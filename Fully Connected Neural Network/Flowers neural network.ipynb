{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializations\n",
    "import numpy as np\n",
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from os import listdir\n",
    "import cv2\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in directory:daisy\n",
      "in directory:dandelion\n",
      "in directory:roses\n",
      "in directory:sunflowers\n",
      "in directory:tulips\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#shuffling the data\\n#np.random.seed(10)  \\ntesting_data,testing_labels=shuffle_in_unison(testing_data,testing_labels)\\ndaisy,dandelion,roses,sunflowers,tulips = np.split(testing_data,5)\\ndaisy_lbls,dandelion_lbls,roses_lbls,sunflowers_lbls,tulips_lbls=np.split(testing_labels,5)\\ntraining_data,training_labels=shuffle_in_unison(training_data,training_labels)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all images in a directory\n",
    "#shuffle the labels and the data in Order\n",
    "def shuffle_in_unison(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "    return a , b\n",
    "\n",
    "def load_directory(f_name):\n",
    "    #print (\"in file \" + f_name)\n",
    "    loaded_images = {}\n",
    "    labels_list = []\n",
    "    loaded_images_list = []\n",
    "    for filename in listdir('flower_photos/' + f_name):\n",
    "        # load image\n",
    "        img_data = plt.imread('flower_photos/' + f_name + '/' + filename)\n",
    "        new_img = cv2.resize(img_data,\n",
    "                             dsize=(32, 32),  # 32 x 32 images\n",
    "                             interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        loaded_images[filename] = new_img\n",
    "        sorted_loaded_images = dict(\n",
    "            sorted(loaded_images.items(), key=lambda x: x[0].lower()))\n",
    "\n",
    "    for key in (sorted_loaded_images):\n",
    "        labels_list.append(f_name)\n",
    "        loaded_images_list.append(loaded_images[key])\n",
    "\n",
    "    ret = np.array(loaded_images_list)\n",
    "    labels = np.array(labels_list)\n",
    "    testing_batch = ret[-100:]\n",
    "    testing_batch_labels = labels[-100:]\n",
    "    validation_batch = ret[-200:-100]\n",
    "    validation_batch_labels = labels[-200:-100]\n",
    "    #plt.pyplot.figure()\n",
    "    #plt.pyplot.imshow(testing_batch[0])\n",
    "    # print (testing_batch.shape)\n",
    "    training_batch = ret[:-200].copy()\n",
    "    training_batch_labels = labels[:-200].copy()\n",
    "    #print(training_batch.shape)\n",
    "\n",
    "    return testing_batch, training_batch, testing_batch_labels, training_batch_labels, validation_batch,validation_batch_labels\n",
    "\n",
    "\n",
    "training_data = []\n",
    "testing_data = []\n",
    "training_labels = []\n",
    "testing_labels = []\n",
    "validation_data = []\n",
    "validation_labels=[]\n",
    "\n",
    "\n",
    "for directoryname in listdir('flower_photos'):\n",
    "    print('in directory:' + directoryname)\n",
    "    testing_batch, training_batch, testing_batch_labels, training_batch_labels,validation_batch,validation_batch_labels = load_directory(\n",
    "        directoryname)\n",
    "    testing_data.extend(testing_batch)\n",
    "    testing_labels.extend(testing_batch_labels)\n",
    "    #plt.figure()\n",
    "    #plt.imshow(testing_batch[0])\n",
    "    training_data.extend(training_batch)\n",
    "    training_labels.extend(training_batch_labels)\n",
    "    \n",
    "    validation_data.extend(validation_batch)\n",
    "    validation_labels.extend(validation_batch_labels)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "testing_data = np.array(testing_data)\n",
    "testing_data = np.reshape(testing_data , (testing_data.shape[0], -1))\n",
    "training_data = np.array(training_data)\n",
    "training_data = np.reshape(training_data , (training_data.shape[0], -1))\n",
    "training_labels = np.array(training_labels)\n",
    "testing_labels = np.array(testing_labels)\n",
    "\n",
    "validation_labels = np.array(validation_labels)\n",
    "validation_data = np.array (validation_data)\n",
    "validation_data = np.reshape(validation_data , (validation_data.shape[0], -1))\n",
    "\n",
    "#preprocess the data (zero centered and normalized)\n",
    "\n",
    "'''\n",
    "def load_flowers ():\n",
    "    return testing_data , training_data ,training_labels, testing_labels \n",
    "'''\n",
    "'''\n",
    "#shuffling the data\n",
    "#np.random.seed(10)  \n",
    "testing_data,testing_labels=shuffle_in_unison(testing_data,testing_labels)\n",
    "daisy,dandelion,roses,sunflowers,tulips = np.split(testing_data,5)\n",
    "daisy_lbls,dandelion_lbls,roses_lbls,sunflowers_lbls,tulips_lbls=np.split(testing_labels,5)\n",
    "training_data,training_labels=shuffle_in_unison(training_data,training_labels)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling the data\n",
    "np.random.seed(10) \n",
    "\n",
    "daisy,dandelion,roses,sunflowers,tulips = np.split(testing_data,5)\n",
    "\n",
    "training_data,training_labels=shuffle_in_unison(training_data,training_labels)\n",
    "validation_data,validation_labels=shuffle_in_unison(validation_data,validation_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "#print (testing_labels)\n",
    "training_labels[training_labels == 'daisy'] = 0\n",
    "training_labels[training_labels == 'dandelion'] = 1\n",
    "training_labels[training_labels == 'roses'] = 2\n",
    "training_labels[training_labels == 'sunflowers'] = 3\n",
    "training_labels[training_labels == 'tulips'] = 4\n",
    "#print(training_labels[:500])\n",
    "#testing_labels = testing_labels.astype('int32')\n",
    "testing_labels[testing_labels == 'daisy'] = 0\n",
    "testing_labels[testing_labels == 'dandelion'] = 1\n",
    "testing_labels[testing_labels == 'roses'] = 2\n",
    "testing_labels[testing_labels == 'sunflowers'] = 3\n",
    "testing_labels[testing_labels == 'tulips'] = 4\n",
    "\n",
    "validation_labels[validation_labels == 'daisy'] = 0\n",
    "validation_labels[validation_labels == 'dandelion'] = 1\n",
    "validation_labels[validation_labels == 'roses'] = 2\n",
    "validation_labels[validation_labels == 'sunflowers'] = 3\n",
    "validation_labels[validation_labels == 'tulips'] = 4\n",
    "\n",
    "\n",
    "training_labels = training_labels.astype('int32')\n",
    "testing_labels = testing_labels.astype('int32')\n",
    "validation_labels = validation_labels.astype('int32')\n",
    "\n",
    "#daisy_lbls,dandelion_lbls,roses_lbls,sunflowers_lbls,tulips_lbls=np.split(testing_labels,5)\n",
    "#testing_data,testing_labels=shuffle_in_unison(testing_data,testing_labels)\n",
    "#print (daisy_lbls)\n",
    "print (testing_labels)\n",
    "print (training_labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "daisy_lbls,dandelion_lbls,roses_lbls,sunflowers_lbls,tulips_lbls=np.split(testing_labels,5)\n",
    "print (daisy_lbls)\n",
    "#testing_data,testing_labels=shuffle_in_unison(testing_data,testing_labels) #this shuffles the splitted data , be careful !!\n",
    "print (daisy_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 3072)\n",
      "(2670, 3072)\n",
      "(500, 3072)\n",
      "(2670,)\n",
      "(500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(testing_data.shape)\n",
    "print(training_data.shape)\n",
    "print(validation_data.shape)\n",
    "print(training_labels.shape)\n",
    "print(testing_labels.shape)\n",
    "print(validation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 4 2 3 4 1 1 0 2 2 3 3 4 4 2 2 4 2 1 0 2 0 2 1 4 1 0 2 3 4 0 0 1 2 1 1\n",
      " 1 3 4 0 4 0 2 2 3 2 1 4 0 1 1 1 4 4 4 1 1 3 2 4 2 1 3 4 2 4 2 3 1 1 1 2 0\n",
      " 3 0 3 0 1 4 1 3 2 2 1 3 3 3 1 4 0 4 3 0 4 2 4 0 0 0 4 2 2 1 1 0 0 3 4 3 0\n",
      " 2 0 3 3 3 0 3 4 3 1 2 0 2 4 0 1 3 4 2 3 4 4 1 1 2 2 0 1 0 4 1 4 3 1 1 0 0\n",
      " 0 3 2 1 3 2 2 3 0 0 2 3 0 4 4 2 2 2 1 0 3 3 2 2 4 0 4 0 3 2 4 0 3 3 2 3 0\n",
      " 3 3 3 4 4 3 3 4 1 4 0 0 4 4 0 3 2 2 2 4 4 4 4 2 2 2 1 2 0 0 2 4 1 1 2 4 4\n",
      " 0 0 3 1 4 3 4 2 1 4 3 4 1 2 4 4 0 1 0 1 1 0 2 3 2 2 0 1 1 1 0 1 4 1 0 1 4\n",
      " 0 1 4 4 2 1 1 1 2 2 3 0 1 1 0 3 4 4 1 3 3 2 0 0 3 2 3 0 3 4 2 1 2 3 0 4 1\n",
      " 3 4 0 4 0 3 1 0 4 4 1 4 1 1 2 0 4 4 2 3 3 1 2 0 4 2 3 2 0 0 2 3 2 0 3 2 3\n",
      " 0 0 0 3 0 4 3 0 1 3 1 1 2 1 0 0 2 4 2 4 1 0 4 1 4 0 0 0 3 2 4 3 1 3 3 1 4\n",
      " 2 4 3 1 0 0 1 1 4 1 2 0 3 2 1 3 4 1 2 3 4 4 3 1 4 4 3 3 2 3 1 1 0 3 3 1 0\n",
      " 4 0 4 1 4 2 3 2 3 3 3 2 1 0 4 1 2 2 0 0 0 3 2 0 2 3 0 2 1 1 2 1 3 4 0 2 1\n",
      " 2 4 0 4 3 1 3 4 0 3 3 1 4 4 0 2 3 1 2 1 0 0 2 0 1 2 4 2 3 1 3 1 0 4 0 3 2\n",
      " 2 3 3 2 1 1 0 2 4 0 4 4 4 2 3 1 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a linear classifier to solve the problem as a start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n"
     ]
    }
   ],
   "source": [
    "#some useful constants\n",
    "w = 32 #image dimenstion\n",
    "K = 5 #number of classes\n",
    "D = w * w * 3 # Dimensionality \n",
    "print (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a Linear Classifier\n",
    "\n",
    "# initialize parameters randomly\n",
    "W =  np.random.randn(D,K)/ np.sqrt(D) #xavier\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-6 # regularization strength\n",
    "\n",
    "X = training_data.astype('float64')\n",
    "y = training_labels\n",
    "\n",
    "#preprocess the data\n",
    "X -= np.mean (X , axis = 0 )\n",
    "#X = X.astype('float')\n",
    "#print (training_data.shape)\n",
    "X /= np.std (X , axis = 0 )\n",
    "X/= 255.0\n",
    "\n",
    "print (X)\n",
    "# gradient descent loop\n",
    "num_examples = training_data.shape[0]\n",
    "all_loss=[]\n",
    "for i in range(10000):\n",
    "\n",
    "    # evaluate class scores, [N x K]\n",
    "    scores = np.dot(X, W) + b\n",
    "    #print(scores)\n",
    "\n",
    "    # compute the class probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    #print (exp_scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(correct_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W)\n",
    "    loss = data_loss + reg_loss\n",
    "    all_loss.append(loss)\n",
    "    if i % 1000 == 0:\n",
    "        print (\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "    # backpropate the gradient to the parameters (W,b)\n",
    "    dW = np.dot(X.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "    dW += reg*W # regularization gradient\n",
    "\n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (np.std(X, axis=0))\n",
    "plt.plot(all_loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate training set accuracy\n",
    "scores = np.dot(X, W) + b\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print ('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate validation set accuracy\n",
    "validation_data = validation_data.astype('float64')\n",
    "#validation_data -= np.mean(validation_data , axis = 0)\n",
    "#validation_data /= np.std(validation_data , axis = 0)\n",
    "\n",
    "scores = np.dot(validation_data, W) + b\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print(predicted_class[:10])\n",
    "print (validation_labels [:10])\n",
    "print ('validation_set accuracy: %.2f' % (np.mean(predicted_class == validation_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Two hidden layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the weights using xavier initializer\n",
    "n_in = D\n",
    "h_1 = 1000 #nodes in the first hidden layer\n",
    "h_2 = 1000 #node in the second hidden layer\"\n",
    "n_out = K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_W = np.array([])\n",
    "Best_W2 = np.array([])\n",
    "Best_W3 =np.array([])\n",
    "Best_b = np.array([])\n",
    "Best_b2 = np.array([])\n",
    "Best_b3 = np.array([])\n",
    "Best_val_acc = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:Tr_loss: 2.607574, val_loss: 2.626760 ,Tr_acc: 0.261798 , val_acc: 0.236000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 1:Tr_loss: 2.595534, val_loss: 2.619260 ,Tr_acc: 0.294382 , val_acc: 0.260000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 2:Tr_loss: 2.584628, val_loss: 2.612244 ,Tr_acc: 0.314607 , val_acc: 0.264000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 3:Tr_loss: 2.574565, val_loss: 2.605531 ,Tr_acc: 0.323221 , val_acc: 0.268000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 4:Tr_loss: 2.565134, val_loss: 2.598995 ,Tr_acc: 0.337453 , val_acc: 0.270000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 5:Tr_loss: 2.556190, val_loss: 2.592579 ,Tr_acc: 0.340449 , val_acc: 0.276000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 6:Tr_loss: 2.547631, val_loss: 2.586241 ,Tr_acc: 0.350187 , val_acc: 0.288000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 7:Tr_loss: 2.539374, val_loss: 2.579949 ,Tr_acc: 0.358052 , val_acc: 0.290000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 8:Tr_loss: 2.531373, val_loss: 2.573708 ,Tr_acc: 0.364794 , val_acc: 0.290000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 9:Tr_loss: 2.523585, val_loss: 2.567505 ,Tr_acc: 0.371161 , val_acc: 0.300000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 10:Tr_loss: 2.515984, val_loss: 2.561350 ,Tr_acc: 0.379401 , val_acc: 0.302000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 11:Tr_loss: 2.508538, val_loss: 2.555236 ,Tr_acc: 0.383895 , val_acc: 0.310000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 12:Tr_loss: 2.501229, val_loss: 2.549151 ,Tr_acc: 0.392135 , val_acc: 0.318000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 13:Tr_loss: 2.494033, val_loss: 2.543112 ,Tr_acc: 0.394382 , val_acc: 0.314000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 14:Tr_loss: 2.486935, val_loss: 2.537124 ,Tr_acc: 0.400000 , val_acc: 0.324000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 15:Tr_loss: 2.479922, val_loss: 2.531182 ,Tr_acc: 0.406367 , val_acc: 0.330000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 16:Tr_loss: 2.472999, val_loss: 2.525301 ,Tr_acc: 0.409363 , val_acc: 0.342000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 17:Tr_loss: 2.466160, val_loss: 2.519478 ,Tr_acc: 0.413109 , val_acc: 0.342000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 18:Tr_loss: 2.459397, val_loss: 2.513718 ,Tr_acc: 0.417228 , val_acc: 0.348000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 19:Tr_loss: 2.452718, val_loss: 2.508030 ,Tr_acc: 0.420974 , val_acc: 0.348000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 20:Tr_loss: 2.446115, val_loss: 2.502413 ,Tr_acc: 0.426217 , val_acc: 0.350000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 21:Tr_loss: 2.439577, val_loss: 2.496863 ,Tr_acc: 0.426217 , val_acc: 0.354000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 22:Tr_loss: 2.433107, val_loss: 2.491368 ,Tr_acc: 0.429588 , val_acc: 0.362000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 23:Tr_loss: 2.426702, val_loss: 2.485940 ,Tr_acc: 0.434831 , val_acc: 0.364000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 24:Tr_loss: 2.420360, val_loss: 2.480572 ,Tr_acc: 0.440449 , val_acc: 0.364000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 25:Tr_loss: 2.414074, val_loss: 2.475270 ,Tr_acc: 0.445318 , val_acc: 0.364000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 26:Tr_loss: 2.407855, val_loss: 2.470037 ,Tr_acc: 0.447566 , val_acc: 0.364000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 27:Tr_loss: 2.401699, val_loss: 2.464873 ,Tr_acc: 0.451685 , val_acc: 0.370000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 28:Tr_loss: 2.395602, val_loss: 2.459781 ,Tr_acc: 0.454682 , val_acc: 0.372000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 29:Tr_loss: 2.389564, val_loss: 2.454761 ,Tr_acc: 0.456554 , val_acc: 0.374000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 30:Tr_loss: 2.383586, val_loss: 2.449808 ,Tr_acc: 0.456929 , val_acc: 0.376000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 31:Tr_loss: 2.377667, val_loss: 2.444931 ,Tr_acc: 0.462921 , val_acc: 0.388000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 32:Tr_loss: 2.371804, val_loss: 2.440124 ,Tr_acc: 0.463296 , val_acc: 0.386000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 33:Tr_loss: 2.366000, val_loss: 2.435391 ,Tr_acc: 0.466667 , val_acc: 0.390000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 34:Tr_loss: 2.360257, val_loss: 2.430721 ,Tr_acc: 0.471161 , val_acc: 0.396000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 35:Tr_loss: 2.354568, val_loss: 2.426124 ,Tr_acc: 0.476779 , val_acc: 0.402000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 36:Tr_loss: 2.348939, val_loss: 2.421598 ,Tr_acc: 0.479401 , val_acc: 0.408000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 37:Tr_loss: 2.343369, val_loss: 2.417143 ,Tr_acc: 0.480150 , val_acc: 0.408000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 38:Tr_loss: 2.337858, val_loss: 2.412761 ,Tr_acc: 0.482397 , val_acc: 0.408000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 39:Tr_loss: 2.332408, val_loss: 2.408450 ,Tr_acc: 0.485019 , val_acc: 0.410000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 40:Tr_loss: 2.327015, val_loss: 2.404207 ,Tr_acc: 0.488390 , val_acc: 0.416000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 41:Tr_loss: 2.321680, val_loss: 2.400037 ,Tr_acc: 0.489139 , val_acc: 0.414000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 42:Tr_loss: 2.316399, val_loss: 2.395942 ,Tr_acc: 0.492135 , val_acc: 0.424000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 43:Tr_loss: 2.311176, val_loss: 2.391920 ,Tr_acc: 0.496255 , val_acc: 0.428000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 44:Tr_loss: 2.306007, val_loss: 2.387969 ,Tr_acc: 0.500375 , val_acc: 0.428000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 45:Tr_loss: 2.300895, val_loss: 2.384090 ,Tr_acc: 0.501873 , val_acc: 0.430000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 46:Tr_loss: 2.295842, val_loss: 2.380289 ,Tr_acc: 0.503745 , val_acc: 0.436000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 47:Tr_loss: 2.290846, val_loss: 2.376560 ,Tr_acc: 0.506742 , val_acc: 0.432000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 48:Tr_loss: 2.285908, val_loss: 2.372901 ,Tr_acc: 0.508989 , val_acc: 0.430000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 49:Tr_loss: 2.281028, val_loss: 2.369309 ,Tr_acc: 0.510487 , val_acc: 0.430000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 50:Tr_loss: 2.276206, val_loss: 2.365786 ,Tr_acc: 0.511610 , val_acc: 0.434000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 51:Tr_loss: 2.271447, val_loss: 2.362333 ,Tr_acc: 0.513483 , val_acc: 0.434000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 52:Tr_loss: 2.266745, val_loss: 2.358949 ,Tr_acc: 0.514981 , val_acc: 0.438000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 53:Tr_loss: 2.262099, val_loss: 2.355641 ,Tr_acc: 0.517228 , val_acc: 0.436000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 54:Tr_loss: 2.257514, val_loss: 2.352404 ,Tr_acc: 0.517603 , val_acc: 0.436000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 55:Tr_loss: 2.252984, val_loss: 2.349241 ,Tr_acc: 0.520599 , val_acc: 0.444000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 56:Tr_loss: 2.248508, val_loss: 2.346146 ,Tr_acc: 0.522472 , val_acc: 0.444000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 57:Tr_loss: 2.244087, val_loss: 2.343116 ,Tr_acc: 0.521723 , val_acc: 0.444000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 58:Tr_loss: 2.239723, val_loss: 2.340152 ,Tr_acc: 0.523596 , val_acc: 0.442000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 59:Tr_loss: 2.235411, val_loss: 2.337255 ,Tr_acc: 0.525094 , val_acc: 0.444000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 60:Tr_loss: 2.231151, val_loss: 2.334423 ,Tr_acc: 0.527341 , val_acc: 0.446000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 61:Tr_loss: 2.226941, val_loss: 2.331654 ,Tr_acc: 0.526592 , val_acc: 0.446000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 62:Tr_loss: 2.222788, val_loss: 2.328943 ,Tr_acc: 0.527715 , val_acc: 0.444000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 63:Tr_loss: 2.218689, val_loss: 2.326292 ,Tr_acc: 0.528464 , val_acc: 0.448000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 64:Tr_loss: 2.214639, val_loss: 2.323704 ,Tr_acc: 0.529963 , val_acc: 0.450000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 65:Tr_loss: 2.210642, val_loss: 2.321175 ,Tr_acc: 0.532959 , val_acc: 0.452000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 66:Tr_loss: 2.206697, val_loss: 2.318703 ,Tr_acc: 0.535955 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 67:Tr_loss: 2.202803, val_loss: 2.316286 ,Tr_acc: 0.537079 , val_acc: 0.452000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 68:Tr_loss: 2.198956, val_loss: 2.313924 ,Tr_acc: 0.540075 , val_acc: 0.452000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 69:Tr_loss: 2.195154, val_loss: 2.311618 ,Tr_acc: 0.540824 , val_acc: 0.452000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 70:Tr_loss: 2.191396, val_loss: 2.309362 ,Tr_acc: 0.541948 , val_acc: 0.452000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 71:Tr_loss: 2.187683, val_loss: 2.307161 ,Tr_acc: 0.543446 , val_acc: 0.452000 , lr: 0.050000 , Reg:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72:Tr_loss: 2.184014, val_loss: 2.305014 ,Tr_acc: 0.544944 , val_acc: 0.456000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 73:Tr_loss: 2.180386, val_loss: 2.302912 ,Tr_acc: 0.546816 , val_acc: 0.460000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 74:Tr_loss: 2.176800, val_loss: 2.300856 ,Tr_acc: 0.547566 , val_acc: 0.458000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 75:Tr_loss: 2.173255, val_loss: 2.298847 ,Tr_acc: 0.548315 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 76:Tr_loss: 2.169749, val_loss: 2.296877 ,Tr_acc: 0.550562 , val_acc: 0.462000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 77:Tr_loss: 2.166282, val_loss: 2.294948 ,Tr_acc: 0.552060 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 78:Tr_loss: 2.162855, val_loss: 2.293066 ,Tr_acc: 0.554682 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 79:Tr_loss: 2.159465, val_loss: 2.291225 ,Tr_acc: 0.556554 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 80:Tr_loss: 2.156110, val_loss: 2.289429 ,Tr_acc: 0.556180 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 81:Tr_loss: 2.152791, val_loss: 2.287674 ,Tr_acc: 0.558052 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 82:Tr_loss: 2.149507, val_loss: 2.285960 ,Tr_acc: 0.559551 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 83:Tr_loss: 2.146254, val_loss: 2.284280 ,Tr_acc: 0.559176 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 84:Tr_loss: 2.143034, val_loss: 2.282634 ,Tr_acc: 0.560300 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 85:Tr_loss: 2.139845, val_loss: 2.281023 ,Tr_acc: 0.559551 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 86:Tr_loss: 2.136686, val_loss: 2.279446 ,Tr_acc: 0.561423 , val_acc: 0.462000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 87:Tr_loss: 2.133557, val_loss: 2.277902 ,Tr_acc: 0.561798 , val_acc: 0.466000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 88:Tr_loss: 2.130460, val_loss: 2.276390 ,Tr_acc: 0.562921 , val_acc: 0.466000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 89:Tr_loss: 2.127390, val_loss: 2.274914 ,Tr_acc: 0.565169 , val_acc: 0.468000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 90:Tr_loss: 2.124348, val_loss: 2.273467 ,Tr_acc: 0.565543 , val_acc: 0.470000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 91:Tr_loss: 2.121330, val_loss: 2.272056 ,Tr_acc: 0.567790 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 92:Tr_loss: 2.118337, val_loss: 2.270675 ,Tr_acc: 0.568914 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 93:Tr_loss: 2.115367, val_loss: 2.269320 ,Tr_acc: 0.571536 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 94:Tr_loss: 2.112421, val_loss: 2.267995 ,Tr_acc: 0.573034 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 95:Tr_loss: 2.109499, val_loss: 2.266698 ,Tr_acc: 0.573408 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 96:Tr_loss: 2.106602, val_loss: 2.265422 ,Tr_acc: 0.575655 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 97:Tr_loss: 2.103727, val_loss: 2.264171 ,Tr_acc: 0.576779 , val_acc: 0.480000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 98:Tr_loss: 2.100872, val_loss: 2.262942 ,Tr_acc: 0.578652 , val_acc: 0.480000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 99:Tr_loss: 2.098037, val_loss: 2.261736 ,Tr_acc: 0.580150 , val_acc: 0.484000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 100:Tr_loss: 2.095221, val_loss: 2.260552 ,Tr_acc: 0.581273 , val_acc: 0.484000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 101:Tr_loss: 2.092426, val_loss: 2.259395 ,Tr_acc: 0.583146 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 102:Tr_loss: 2.089649, val_loss: 2.258262 ,Tr_acc: 0.584644 , val_acc: 0.484000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 103:Tr_loss: 2.086891, val_loss: 2.257141 ,Tr_acc: 0.586142 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 104:Tr_loss: 2.084151, val_loss: 2.256050 ,Tr_acc: 0.588390 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 105:Tr_loss: 2.081428, val_loss: 2.254977 ,Tr_acc: 0.589139 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 106:Tr_loss: 2.078721, val_loss: 2.253923 ,Tr_acc: 0.591011 , val_acc: 0.488000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 107:Tr_loss: 2.076029, val_loss: 2.252889 ,Tr_acc: 0.594007 , val_acc: 0.488000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 108:Tr_loss: 2.073353, val_loss: 2.251873 ,Tr_acc: 0.595506 , val_acc: 0.488000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 109:Tr_loss: 2.070691, val_loss: 2.250878 ,Tr_acc: 0.596255 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 110:Tr_loss: 2.068043, val_loss: 2.249890 ,Tr_acc: 0.598502 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 111:Tr_loss: 2.065410, val_loss: 2.248922 ,Tr_acc: 0.599251 , val_acc: 0.482000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 112:Tr_loss: 2.062787, val_loss: 2.247969 ,Tr_acc: 0.600000 , val_acc: 0.482000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 113:Tr_loss: 2.060177, val_loss: 2.247028 ,Tr_acc: 0.601124 , val_acc: 0.482000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 114:Tr_loss: 2.057577, val_loss: 2.246107 ,Tr_acc: 0.602996 , val_acc: 0.484000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 115:Tr_loss: 2.054988, val_loss: 2.245206 ,Tr_acc: 0.604869 , val_acc: 0.484000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 116:Tr_loss: 2.052410, val_loss: 2.244320 ,Tr_acc: 0.605993 , val_acc: 0.484000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 117:Tr_loss: 2.049843, val_loss: 2.243448 ,Tr_acc: 0.607116 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 118:Tr_loss: 2.047286, val_loss: 2.242591 ,Tr_acc: 0.607865 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 119:Tr_loss: 2.044738, val_loss: 2.241746 ,Tr_acc: 0.609738 , val_acc: 0.488000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 120:Tr_loss: 2.042199, val_loss: 2.240917 ,Tr_acc: 0.610861 , val_acc: 0.488000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 121:Tr_loss: 2.039669, val_loss: 2.240094 ,Tr_acc: 0.612734 , val_acc: 0.488000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 122:Tr_loss: 2.037149, val_loss: 2.239287 ,Tr_acc: 0.614232 , val_acc: 0.488000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 123:Tr_loss: 2.034638, val_loss: 2.238485 ,Tr_acc: 0.614607 , val_acc: 0.488000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 124:Tr_loss: 2.032135, val_loss: 2.237696 ,Tr_acc: 0.615356 , val_acc: 0.488000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 125:Tr_loss: 2.029639, val_loss: 2.236913 ,Tr_acc: 0.616479 , val_acc: 0.488000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 126:Tr_loss: 2.027149, val_loss: 2.236143 ,Tr_acc: 0.616854 , val_acc: 0.488000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 127:Tr_loss: 2.024667, val_loss: 2.235389 ,Tr_acc: 0.617978 , val_acc: 0.492000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 128:Tr_loss: 2.022192, val_loss: 2.234647 ,Tr_acc: 0.619101 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 129:Tr_loss: 2.019723, val_loss: 2.233917 ,Tr_acc: 0.619850 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 130:Tr_loss: 2.017259, val_loss: 2.233190 ,Tr_acc: 0.619476 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 131:Tr_loss: 2.014800, val_loss: 2.232481 ,Tr_acc: 0.622846 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 132:Tr_loss: 2.012348, val_loss: 2.231781 ,Tr_acc: 0.623596 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 133:Tr_loss: 2.009902, val_loss: 2.231089 ,Tr_acc: 0.623596 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 134:Tr_loss: 2.007461, val_loss: 2.230409 ,Tr_acc: 0.625094 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 135:Tr_loss: 2.005025, val_loss: 2.229745 ,Tr_acc: 0.626592 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 136:Tr_loss: 2.002593, val_loss: 2.229088 ,Tr_acc: 0.628090 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 137:Tr_loss: 2.000167, val_loss: 2.228444 ,Tr_acc: 0.628839 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 138:Tr_loss: 1.997745, val_loss: 2.227805 ,Tr_acc: 0.629963 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 139:Tr_loss: 1.995327, val_loss: 2.227178 ,Tr_acc: 0.631086 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 140:Tr_loss: 1.992912, val_loss: 2.226556 ,Tr_acc: 0.632584 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 141:Tr_loss: 1.990501, val_loss: 2.225943 ,Tr_acc: 0.633708 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 142:Tr_loss: 1.988094, val_loss: 2.225331 ,Tr_acc: 0.634457 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143:Tr_loss: 1.985690, val_loss: 2.224734 ,Tr_acc: 0.635206 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 144:Tr_loss: 1.983287, val_loss: 2.224133 ,Tr_acc: 0.637828 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 145:Tr_loss: 1.980887, val_loss: 2.223536 ,Tr_acc: 0.638202 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 146:Tr_loss: 1.978487, val_loss: 2.222952 ,Tr_acc: 0.640824 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 147:Tr_loss: 1.976089, val_loss: 2.222375 ,Tr_acc: 0.641199 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 148:Tr_loss: 1.973691, val_loss: 2.221801 ,Tr_acc: 0.641948 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 149:Tr_loss: 1.971294, val_loss: 2.221236 ,Tr_acc: 0.643446 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 150:Tr_loss: 1.968898, val_loss: 2.220681 ,Tr_acc: 0.645693 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 151:Tr_loss: 1.966504, val_loss: 2.220133 ,Tr_acc: 0.646816 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 152:Tr_loss: 1.964112, val_loss: 2.219583 ,Tr_acc: 0.647566 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 153:Tr_loss: 1.961721, val_loss: 2.219049 ,Tr_acc: 0.647940 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 154:Tr_loss: 1.959330, val_loss: 2.218512 ,Tr_acc: 0.649813 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 155:Tr_loss: 1.956939, val_loss: 2.217984 ,Tr_acc: 0.650187 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 156:Tr_loss: 1.954548, val_loss: 2.217458 ,Tr_acc: 0.650936 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 157:Tr_loss: 1.952157, val_loss: 2.216951 ,Tr_acc: 0.651685 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 158:Tr_loss: 1.949767, val_loss: 2.216449 ,Tr_acc: 0.653933 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 159:Tr_loss: 1.947379, val_loss: 2.215944 ,Tr_acc: 0.656554 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 160:Tr_loss: 1.944990, val_loss: 2.215441 ,Tr_acc: 0.656929 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 161:Tr_loss: 1.942602, val_loss: 2.214940 ,Tr_acc: 0.658427 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 162:Tr_loss: 1.940213, val_loss: 2.214445 ,Tr_acc: 0.659551 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 163:Tr_loss: 1.937825, val_loss: 2.213963 ,Tr_acc: 0.661798 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 164:Tr_loss: 1.935437, val_loss: 2.213473 ,Tr_acc: 0.663296 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 165:Tr_loss: 1.933050, val_loss: 2.212996 ,Tr_acc: 0.665169 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 166:Tr_loss: 1.930662, val_loss: 2.212517 ,Tr_acc: 0.665543 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 167:Tr_loss: 1.928274, val_loss: 2.212054 ,Tr_acc: 0.666667 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 168:Tr_loss: 1.925886, val_loss: 2.211583 ,Tr_acc: 0.667790 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 169:Tr_loss: 1.923497, val_loss: 2.211127 ,Tr_acc: 0.669288 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 170:Tr_loss: 1.921107, val_loss: 2.210659 ,Tr_acc: 0.670412 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 171:Tr_loss: 1.918716, val_loss: 2.210201 ,Tr_acc: 0.671536 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 172:Tr_loss: 1.916324, val_loss: 2.209735 ,Tr_acc: 0.674157 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 173:Tr_loss: 1.913930, val_loss: 2.209279 ,Tr_acc: 0.676779 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 174:Tr_loss: 1.911536, val_loss: 2.208828 ,Tr_acc: 0.678652 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 175:Tr_loss: 1.909141, val_loss: 2.208383 ,Tr_acc: 0.681273 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 176:Tr_loss: 1.906745, val_loss: 2.207941 ,Tr_acc: 0.681648 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 177:Tr_loss: 1.904349, val_loss: 2.207505 ,Tr_acc: 0.685768 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 178:Tr_loss: 1.901952, val_loss: 2.207066 ,Tr_acc: 0.686891 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 179:Tr_loss: 1.899555, val_loss: 2.206636 ,Tr_acc: 0.688390 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 180:Tr_loss: 1.897156, val_loss: 2.206208 ,Tr_acc: 0.690637 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 181:Tr_loss: 1.894754, val_loss: 2.205782 ,Tr_acc: 0.691386 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 182:Tr_loss: 1.892352, val_loss: 2.205358 ,Tr_acc: 0.692884 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 183:Tr_loss: 1.889947, val_loss: 2.204940 ,Tr_acc: 0.692884 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 184:Tr_loss: 1.887541, val_loss: 2.204527 ,Tr_acc: 0.693633 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 185:Tr_loss: 1.885134, val_loss: 2.204122 ,Tr_acc: 0.694757 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 186:Tr_loss: 1.882724, val_loss: 2.203715 ,Tr_acc: 0.695880 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 187:Tr_loss: 1.880312, val_loss: 2.203315 ,Tr_acc: 0.698127 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 188:Tr_loss: 1.877898, val_loss: 2.202923 ,Tr_acc: 0.699625 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 189:Tr_loss: 1.875483, val_loss: 2.202541 ,Tr_acc: 0.700375 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 190:Tr_loss: 1.873066, val_loss: 2.202157 ,Tr_acc: 0.702622 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 191:Tr_loss: 1.870648, val_loss: 2.201779 ,Tr_acc: 0.703371 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 192:Tr_loss: 1.868229, val_loss: 2.201411 ,Tr_acc: 0.704869 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 193:Tr_loss: 1.865808, val_loss: 2.201037 ,Tr_acc: 0.707865 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 194:Tr_loss: 1.863386, val_loss: 2.200694 ,Tr_acc: 0.710487 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 195:Tr_loss: 1.860962, val_loss: 2.200316 ,Tr_acc: 0.711985 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 196:Tr_loss: 1.858535, val_loss: 2.199964 ,Tr_acc: 0.713109 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 197:Tr_loss: 1.856107, val_loss: 2.199600 ,Tr_acc: 0.714232 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 198:Tr_loss: 1.853677, val_loss: 2.199241 ,Tr_acc: 0.715730 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 199:Tr_loss: 1.851245, val_loss: 2.198891 ,Tr_acc: 0.716854 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 200:Tr_loss: 1.848810, val_loss: 2.198553 ,Tr_acc: 0.718727 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 201:Tr_loss: 1.846373, val_loss: 2.198212 ,Tr_acc: 0.720974 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 202:Tr_loss: 1.843935, val_loss: 2.197869 ,Tr_acc: 0.722097 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 203:Tr_loss: 1.841495, val_loss: 2.197545 ,Tr_acc: 0.722846 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 204:Tr_loss: 1.839052, val_loss: 2.197223 ,Tr_acc: 0.725094 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 205:Tr_loss: 1.836607, val_loss: 2.196892 ,Tr_acc: 0.725843 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 206:Tr_loss: 1.834160, val_loss: 2.196589 ,Tr_acc: 0.727341 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 207:Tr_loss: 1.831709, val_loss: 2.196267 ,Tr_acc: 0.728839 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 208:Tr_loss: 1.829257, val_loss: 2.195959 ,Tr_acc: 0.734082 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 209:Tr_loss: 1.826801, val_loss: 2.195655 ,Tr_acc: 0.735581 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 210:Tr_loss: 1.824343, val_loss: 2.195343 ,Tr_acc: 0.737079 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 211:Tr_loss: 1.821884, val_loss: 2.195044 ,Tr_acc: 0.737828 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 212:Tr_loss: 1.819422, val_loss: 2.194740 ,Tr_acc: 0.738577 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 213:Tr_loss: 1.816958, val_loss: 2.194460 ,Tr_acc: 0.739700 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214:Tr_loss: 1.814491, val_loss: 2.194162 ,Tr_acc: 0.741199 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 215:Tr_loss: 1.812023, val_loss: 2.193885 ,Tr_acc: 0.741573 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 216:Tr_loss: 1.809554, val_loss: 2.193591 ,Tr_acc: 0.743446 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 217:Tr_loss: 1.807083, val_loss: 2.193323 ,Tr_acc: 0.745693 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 218:Tr_loss: 1.804609, val_loss: 2.193028 ,Tr_acc: 0.746816 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 219:Tr_loss: 1.802135, val_loss: 2.192755 ,Tr_acc: 0.747566 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 220:Tr_loss: 1.799659, val_loss: 2.192477 ,Tr_acc: 0.749064 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 221:Tr_loss: 1.797181, val_loss: 2.192210 ,Tr_acc: 0.750562 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 222:Tr_loss: 1.794700, val_loss: 2.191938 ,Tr_acc: 0.751685 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 223:Tr_loss: 1.792216, val_loss: 2.191681 ,Tr_acc: 0.752809 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 224:Tr_loss: 1.789730, val_loss: 2.191422 ,Tr_acc: 0.753558 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 225:Tr_loss: 1.787241, val_loss: 2.191153 ,Tr_acc: 0.754682 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 226:Tr_loss: 1.784749, val_loss: 2.190916 ,Tr_acc: 0.755805 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 227:Tr_loss: 1.782255, val_loss: 2.190669 ,Tr_acc: 0.757678 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 228:Tr_loss: 1.779760, val_loss: 2.190420 ,Tr_acc: 0.758801 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 229:Tr_loss: 1.777262, val_loss: 2.190175 ,Tr_acc: 0.760300 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 230:Tr_loss: 1.774763, val_loss: 2.189932 ,Tr_acc: 0.762921 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 231:Tr_loss: 1.772261, val_loss: 2.189694 ,Tr_acc: 0.765169 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 232:Tr_loss: 1.769758, val_loss: 2.189440 ,Tr_acc: 0.766667 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 233:Tr_loss: 1.767252, val_loss: 2.189214 ,Tr_acc: 0.768165 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 234:Tr_loss: 1.764743, val_loss: 2.188970 ,Tr_acc: 0.770037 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 235:Tr_loss: 1.762233, val_loss: 2.188745 ,Tr_acc: 0.771161 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 236:Tr_loss: 1.759721, val_loss: 2.188524 ,Tr_acc: 0.771910 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 237:Tr_loss: 1.757206, val_loss: 2.188305 ,Tr_acc: 0.773034 , val_acc: 0.528000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 238:Tr_loss: 1.754690, val_loss: 2.188085 ,Tr_acc: 0.774157 , val_acc: 0.528000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 239:Tr_loss: 1.752172, val_loss: 2.187865 ,Tr_acc: 0.774906 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 240:Tr_loss: 1.749652, val_loss: 2.187645 ,Tr_acc: 0.775281 , val_acc: 0.528000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 241:Tr_loss: 1.747130, val_loss: 2.187438 ,Tr_acc: 0.775655 , val_acc: 0.528000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 242:Tr_loss: 1.744606, val_loss: 2.187236 ,Tr_acc: 0.777154 , val_acc: 0.528000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 243:Tr_loss: 1.742081, val_loss: 2.187037 ,Tr_acc: 0.780524 , val_acc: 0.528000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 244:Tr_loss: 1.739554, val_loss: 2.186840 ,Tr_acc: 0.782022 , val_acc: 0.528000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 245:Tr_loss: 1.737023, val_loss: 2.186644 ,Tr_acc: 0.782772 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 246:Tr_loss: 1.734491, val_loss: 2.186455 ,Tr_acc: 0.784644 , val_acc: 0.528000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 247:Tr_loss: 1.731959, val_loss: 2.186249 ,Tr_acc: 0.786142 , val_acc: 0.528000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 248:Tr_loss: 1.729426, val_loss: 2.186059 ,Tr_acc: 0.788015 , val_acc: 0.528000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 249:Tr_loss: 1.726892, val_loss: 2.185877 ,Tr_acc: 0.789513 , val_acc: 0.528000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 250:Tr_loss: 1.724357, val_loss: 2.185694 ,Tr_acc: 0.789513 , val_acc: 0.532000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 251:Tr_loss: 1.721822, val_loss: 2.185513 ,Tr_acc: 0.790262 , val_acc: 0.532000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 252:Tr_loss: 1.719286, val_loss: 2.185351 ,Tr_acc: 0.792135 , val_acc: 0.530000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 253:Tr_loss: 1.716749, val_loss: 2.185190 ,Tr_acc: 0.795131 , val_acc: 0.530000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 254:Tr_loss: 1.714212, val_loss: 2.185022 ,Tr_acc: 0.797378 , val_acc: 0.530000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 255:Tr_loss: 1.711672, val_loss: 2.184865 ,Tr_acc: 0.798127 , val_acc: 0.528000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 256:Tr_loss: 1.709132, val_loss: 2.184716 ,Tr_acc: 0.799625 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 257:Tr_loss: 1.706592, val_loss: 2.184547 ,Tr_acc: 0.800749 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 258:Tr_loss: 1.704052, val_loss: 2.184427 ,Tr_acc: 0.801873 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 259:Tr_loss: 1.701512, val_loss: 2.184267 ,Tr_acc: 0.802996 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 260:Tr_loss: 1.698970, val_loss: 2.184138 ,Tr_acc: 0.805243 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 261:Tr_loss: 1.696427, val_loss: 2.184006 ,Tr_acc: 0.805993 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 262:Tr_loss: 1.693885, val_loss: 2.183871 ,Tr_acc: 0.805993 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 263:Tr_loss: 1.691341, val_loss: 2.183738 ,Tr_acc: 0.806742 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 264:Tr_loss: 1.688797, val_loss: 2.183640 ,Tr_acc: 0.806742 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 265:Tr_loss: 1.686252, val_loss: 2.183504 ,Tr_acc: 0.808989 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 266:Tr_loss: 1.683707, val_loss: 2.183410 ,Tr_acc: 0.810861 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 267:Tr_loss: 1.681164, val_loss: 2.183288 ,Tr_acc: 0.812734 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 268:Tr_loss: 1.678620, val_loss: 2.183201 ,Tr_acc: 0.814232 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 269:Tr_loss: 1.676076, val_loss: 2.183088 ,Tr_acc: 0.816479 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 270:Tr_loss: 1.673533, val_loss: 2.183011 ,Tr_acc: 0.816854 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 271:Tr_loss: 1.670989, val_loss: 2.182909 ,Tr_acc: 0.818352 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 272:Tr_loss: 1.668446, val_loss: 2.182812 ,Tr_acc: 0.819476 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 273:Tr_loss: 1.665902, val_loss: 2.182748 ,Tr_acc: 0.820225 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 274:Tr_loss: 1.663358, val_loss: 2.182642 ,Tr_acc: 0.820974 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 275:Tr_loss: 1.660814, val_loss: 2.182582 ,Tr_acc: 0.822097 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 276:Tr_loss: 1.658269, val_loss: 2.182491 ,Tr_acc: 0.823596 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 277:Tr_loss: 1.655724, val_loss: 2.182438 ,Tr_acc: 0.825843 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 278:Tr_loss: 1.653178, val_loss: 2.182349 ,Tr_acc: 0.826217 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 279:Tr_loss: 1.650632, val_loss: 2.182302 ,Tr_acc: 0.828090 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 280:Tr_loss: 1.648087, val_loss: 2.182224 ,Tr_acc: 0.829213 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 281:Tr_loss: 1.645543, val_loss: 2.182185 ,Tr_acc: 0.829963 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 282:Tr_loss: 1.643000, val_loss: 2.182118 ,Tr_acc: 0.830712 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 283:Tr_loss: 1.640458, val_loss: 2.182091 ,Tr_acc: 0.832210 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 284:Tr_loss: 1.637917, val_loss: 2.182031 ,Tr_acc: 0.832959 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285:Tr_loss: 1.635377, val_loss: 2.182015 ,Tr_acc: 0.832959 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 286:Tr_loss: 1.632838, val_loss: 2.181983 ,Tr_acc: 0.834082 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 287:Tr_loss: 1.630301, val_loss: 2.181966 ,Tr_acc: 0.834082 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 288:Tr_loss: 1.627765, val_loss: 2.181927 ,Tr_acc: 0.836330 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 289:Tr_loss: 1.625231, val_loss: 2.181910 ,Tr_acc: 0.837079 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 290:Tr_loss: 1.622697, val_loss: 2.181892 ,Tr_acc: 0.838202 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 291:Tr_loss: 1.620165, val_loss: 2.181883 ,Tr_acc: 0.838951 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 292:Tr_loss: 1.617633, val_loss: 2.181887 ,Tr_acc: 0.840075 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 293:Tr_loss: 1.615103, val_loss: 2.181883 ,Tr_acc: 0.840824 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 294:Tr_loss: 1.612575, val_loss: 2.181875 ,Tr_acc: 0.841199 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 295:Tr_loss: 1.610049, val_loss: 2.181885 ,Tr_acc: 0.842322 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 296:Tr_loss: 1.607524, val_loss: 2.181885 ,Tr_acc: 0.843446 , val_acc: 0.524000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 297:Tr_loss: 1.605003, val_loss: 2.181911 ,Tr_acc: 0.844944 , val_acc: 0.526000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 298:Tr_loss: 1.602483, val_loss: 2.181918 ,Tr_acc: 0.846816 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 299:Tr_loss: 1.599964, val_loss: 2.181964 ,Tr_acc: 0.847566 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 300:Tr_loss: 1.597448, val_loss: 2.181969 ,Tr_acc: 0.848315 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 301:Tr_loss: 1.594932, val_loss: 2.182047 ,Tr_acc: 0.849438 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 302:Tr_loss: 1.592419, val_loss: 2.182038 ,Tr_acc: 0.851685 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 303:Tr_loss: 1.589909, val_loss: 2.182132 ,Tr_acc: 0.852060 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 304:Tr_loss: 1.587401, val_loss: 2.182133 ,Tr_acc: 0.852809 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 305:Tr_loss: 1.584895, val_loss: 2.182253 ,Tr_acc: 0.853933 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 306:Tr_loss: 1.582390, val_loss: 2.182245 ,Tr_acc: 0.855431 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 307:Tr_loss: 1.579888, val_loss: 2.182383 ,Tr_acc: 0.856554 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 308:Tr_loss: 1.577389, val_loss: 2.182380 ,Tr_acc: 0.857678 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 309:Tr_loss: 1.574892, val_loss: 2.182534 ,Tr_acc: 0.858801 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 310:Tr_loss: 1.572399, val_loss: 2.182520 ,Tr_acc: 0.860300 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 311:Tr_loss: 1.569909, val_loss: 2.182688 ,Tr_acc: 0.861049 , val_acc: 0.522000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 312:Tr_loss: 1.567420, val_loss: 2.182690 ,Tr_acc: 0.863296 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 313:Tr_loss: 1.564933, val_loss: 2.182883 ,Tr_acc: 0.865169 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 314:Tr_loss: 1.562449, val_loss: 2.182881 ,Tr_acc: 0.865918 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 315:Tr_loss: 1.559967, val_loss: 2.183105 ,Tr_acc: 0.866667 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 316:Tr_loss: 1.557488, val_loss: 2.183102 ,Tr_acc: 0.867790 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 317:Tr_loss: 1.555012, val_loss: 2.183331 ,Tr_acc: 0.867790 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 318:Tr_loss: 1.552538, val_loss: 2.183350 ,Tr_acc: 0.868539 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 319:Tr_loss: 1.550068, val_loss: 2.183596 ,Tr_acc: 0.869663 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 320:Tr_loss: 1.547600, val_loss: 2.183601 ,Tr_acc: 0.870412 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 321:Tr_loss: 1.545136, val_loss: 2.183917 ,Tr_acc: 0.870412 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 322:Tr_loss: 1.542673, val_loss: 2.183878 ,Tr_acc: 0.873034 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 323:Tr_loss: 1.540213, val_loss: 2.184198 ,Tr_acc: 0.873408 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 324:Tr_loss: 1.537757, val_loss: 2.184163 ,Tr_acc: 0.873783 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 325:Tr_loss: 1.535305, val_loss: 2.184488 ,Tr_acc: 0.873783 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 326:Tr_loss: 1.532857, val_loss: 2.184483 ,Tr_acc: 0.874906 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 327:Tr_loss: 1.530413, val_loss: 2.184817 ,Tr_acc: 0.875655 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 328:Tr_loss: 1.527972, val_loss: 2.184830 ,Tr_acc: 0.876404 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 329:Tr_loss: 1.525537, val_loss: 2.185184 ,Tr_acc: 0.877154 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 330:Tr_loss: 1.523105, val_loss: 2.185188 ,Tr_acc: 0.878652 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 331:Tr_loss: 1.520677, val_loss: 2.185586 ,Tr_acc: 0.880150 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 332:Tr_loss: 1.518253, val_loss: 2.185580 ,Tr_acc: 0.881648 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 333:Tr_loss: 1.515834, val_loss: 2.186020 ,Tr_acc: 0.882772 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 334:Tr_loss: 1.513418, val_loss: 2.185969 ,Tr_acc: 0.882772 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 335:Tr_loss: 1.511007, val_loss: 2.186440 ,Tr_acc: 0.883895 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 336:Tr_loss: 1.508598, val_loss: 2.186410 ,Tr_acc: 0.884270 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 337:Tr_loss: 1.506194, val_loss: 2.186930 ,Tr_acc: 0.884270 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 338:Tr_loss: 1.503793, val_loss: 2.186876 ,Tr_acc: 0.885768 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 339:Tr_loss: 1.501398, val_loss: 2.187414 ,Tr_acc: 0.887266 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 340:Tr_loss: 1.499007, val_loss: 2.187351 ,Tr_acc: 0.887266 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 341:Tr_loss: 1.496620, val_loss: 2.187917 ,Tr_acc: 0.890637 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 342:Tr_loss: 1.494239, val_loss: 2.187870 ,Tr_acc: 0.891386 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 343:Tr_loss: 1.491862, val_loss: 2.188417 ,Tr_acc: 0.892509 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 344:Tr_loss: 1.489489, val_loss: 2.188389 ,Tr_acc: 0.894007 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 345:Tr_loss: 1.487121, val_loss: 2.188957 ,Tr_acc: 0.895131 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 346:Tr_loss: 1.484759, val_loss: 2.188935 ,Tr_acc: 0.895131 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 347:Tr_loss: 1.482401, val_loss: 2.189523 ,Tr_acc: 0.896255 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 348:Tr_loss: 1.480049, val_loss: 2.189508 ,Tr_acc: 0.896629 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 349:Tr_loss: 1.477699, val_loss: 2.190125 ,Tr_acc: 0.898876 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 350:Tr_loss: 1.475356, val_loss: 2.190110 ,Tr_acc: 0.899625 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 351:Tr_loss: 1.473019, val_loss: 2.190730 ,Tr_acc: 0.901124 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 352:Tr_loss: 1.470688, val_loss: 2.190694 ,Tr_acc: 0.902622 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 353:Tr_loss: 1.468362, val_loss: 2.191374 ,Tr_acc: 0.902996 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 354:Tr_loss: 1.466043, val_loss: 2.191295 ,Tr_acc: 0.902996 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 355:Tr_loss: 1.463728, val_loss: 2.192058 ,Tr_acc: 0.904494 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 356:Tr_loss: 1.461417, val_loss: 2.191932 ,Tr_acc: 0.905243 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 357:Tr_loss: 1.459112, val_loss: 2.192683 ,Tr_acc: 0.907116 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 358:Tr_loss: 1.456813, val_loss: 2.192592 ,Tr_acc: 0.908614 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 359:Tr_loss: 1.454520, val_loss: 2.193360 ,Tr_acc: 0.908989 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 360:Tr_loss: 1.452234, val_loss: 2.193248 ,Tr_acc: 0.909738 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 361:Tr_loss: 1.449952, val_loss: 2.194027 ,Tr_acc: 0.910861 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 362:Tr_loss: 1.447677, val_loss: 2.193928 ,Tr_acc: 0.911985 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 363:Tr_loss: 1.445407, val_loss: 2.194706 ,Tr_acc: 0.911610 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 364:Tr_loss: 1.443142, val_loss: 2.194590 ,Tr_acc: 0.913483 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 365:Tr_loss: 1.440884, val_loss: 2.195436 ,Tr_acc: 0.913483 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 366:Tr_loss: 1.438632, val_loss: 2.195275 ,Tr_acc: 0.914607 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 367:Tr_loss: 1.436385, val_loss: 2.196161 ,Tr_acc: 0.915356 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 368:Tr_loss: 1.434143, val_loss: 2.195970 ,Tr_acc: 0.915730 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 369:Tr_loss: 1.431908, val_loss: 2.196927 ,Tr_acc: 0.917228 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 370:Tr_loss: 1.429678, val_loss: 2.196718 ,Tr_acc: 0.917228 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 371:Tr_loss: 1.427454, val_loss: 2.197682 ,Tr_acc: 0.917978 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 372:Tr_loss: 1.425238, val_loss: 2.197507 ,Tr_acc: 0.917603 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 373:Tr_loss: 1.423026, val_loss: 2.198491 ,Tr_acc: 0.918727 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 374:Tr_loss: 1.420821, val_loss: 2.198269 ,Tr_acc: 0.918727 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 375:Tr_loss: 1.418623, val_loss: 2.199324 ,Tr_acc: 0.919476 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 376:Tr_loss: 1.416431, val_loss: 2.199092 ,Tr_acc: 0.921723 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 377:Tr_loss: 1.414246, val_loss: 2.200157 ,Tr_acc: 0.921348 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 378:Tr_loss: 1.412067, val_loss: 2.199940 ,Tr_acc: 0.922097 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 379:Tr_loss: 1.409894, val_loss: 2.201026 ,Tr_acc: 0.923970 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 380:Tr_loss: 1.407727, val_loss: 2.200797 ,Tr_acc: 0.925468 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 381:Tr_loss: 1.405567, val_loss: 2.201947 ,Tr_acc: 0.925094 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 382:Tr_loss: 1.403416, val_loss: 2.201659 ,Tr_acc: 0.927341 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 383:Tr_loss: 1.401269, val_loss: 2.202869 ,Tr_acc: 0.927341 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 384:Tr_loss: 1.399126, val_loss: 2.202548 ,Tr_acc: 0.928090 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 385:Tr_loss: 1.396991, val_loss: 2.203770 ,Tr_acc: 0.928464 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 386:Tr_loss: 1.394864, val_loss: 2.203455 ,Tr_acc: 0.929963 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 387:Tr_loss: 1.392743, val_loss: 2.204663 ,Tr_acc: 0.929963 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 388:Tr_loss: 1.390629, val_loss: 2.204344 ,Tr_acc: 0.931086 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 389:Tr_loss: 1.388522, val_loss: 2.205609 ,Tr_acc: 0.931461 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 390:Tr_loss: 1.386421, val_loss: 2.205263 ,Tr_acc: 0.933708 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 391:Tr_loss: 1.384327, val_loss: 2.206575 ,Tr_acc: 0.934457 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 392:Tr_loss: 1.382241, val_loss: 2.206212 ,Tr_acc: 0.935955 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 393:Tr_loss: 1.380162, val_loss: 2.207528 ,Tr_acc: 0.935955 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 394:Tr_loss: 1.378090, val_loss: 2.207158 ,Tr_acc: 0.937828 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 395:Tr_loss: 1.376025, val_loss: 2.208544 ,Tr_acc: 0.937079 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 396:Tr_loss: 1.373966, val_loss: 2.208146 ,Tr_acc: 0.938577 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 397:Tr_loss: 1.371913, val_loss: 2.209530 ,Tr_acc: 0.939326 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 398:Tr_loss: 1.369868, val_loss: 2.209102 ,Tr_acc: 0.941199 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 399:Tr_loss: 1.367830, val_loss: 2.210530 ,Tr_acc: 0.940449 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 400:Tr_loss: 1.365799, val_loss: 2.210117 ,Tr_acc: 0.942697 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 401:Tr_loss: 1.363775, val_loss: 2.211569 ,Tr_acc: 0.942322 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 402:Tr_loss: 1.361760, val_loss: 2.211095 ,Tr_acc: 0.944195 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 403:Tr_loss: 1.359750, val_loss: 2.212614 ,Tr_acc: 0.944569 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 404:Tr_loss: 1.357746, val_loss: 2.212105 ,Tr_acc: 0.945318 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 405:Tr_loss: 1.355750, val_loss: 2.213624 ,Tr_acc: 0.946067 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 406:Tr_loss: 1.353762, val_loss: 2.213159 ,Tr_acc: 0.947566 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 407:Tr_loss: 1.351781, val_loss: 2.214705 ,Tr_acc: 0.948689 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 408:Tr_loss: 1.349809, val_loss: 2.214198 ,Tr_acc: 0.947940 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 409:Tr_loss: 1.347843, val_loss: 2.215797 ,Tr_acc: 0.949438 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 410:Tr_loss: 1.345886, val_loss: 2.215260 ,Tr_acc: 0.949064 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 411:Tr_loss: 1.343934, val_loss: 2.216912 ,Tr_acc: 0.949813 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 412:Tr_loss: 1.341989, val_loss: 2.216348 ,Tr_acc: 0.949813 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 413:Tr_loss: 1.340052, val_loss: 2.218022 ,Tr_acc: 0.950562 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 414:Tr_loss: 1.338124, val_loss: 2.217436 ,Tr_acc: 0.950562 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 415:Tr_loss: 1.336201, val_loss: 2.219179 ,Tr_acc: 0.950936 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 416:Tr_loss: 1.334287, val_loss: 2.218537 ,Tr_acc: 0.950936 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 417:Tr_loss: 1.332379, val_loss: 2.220357 ,Tr_acc: 0.951685 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 418:Tr_loss: 1.330480, val_loss: 2.219659 ,Tr_acc: 0.952060 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 419:Tr_loss: 1.328585, val_loss: 2.221507 ,Tr_acc: 0.953558 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 420:Tr_loss: 1.326700, val_loss: 2.220815 ,Tr_acc: 0.952809 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 421:Tr_loss: 1.324821, val_loss: 2.222695 ,Tr_acc: 0.953933 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 422:Tr_loss: 1.322951, val_loss: 2.221974 ,Tr_acc: 0.954307 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 423:Tr_loss: 1.321087, val_loss: 2.223887 ,Tr_acc: 0.954307 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 424:Tr_loss: 1.319232, val_loss: 2.223128 ,Tr_acc: 0.955431 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 425:Tr_loss: 1.317381, val_loss: 2.225058 ,Tr_acc: 0.955805 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 426:Tr_loss: 1.315543, val_loss: 2.224357 ,Tr_acc: 0.955805 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427:Tr_loss: 1.313709, val_loss: 2.226288 ,Tr_acc: 0.956554 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 428:Tr_loss: 1.311887, val_loss: 2.225542 ,Tr_acc: 0.956554 , val_acc: 0.520000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 429:Tr_loss: 1.310071, val_loss: 2.227543 ,Tr_acc: 0.957678 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 430:Tr_loss: 1.308261, val_loss: 2.226727 ,Tr_acc: 0.957678 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 431:Tr_loss: 1.306460, val_loss: 2.228820 ,Tr_acc: 0.958052 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 432:Tr_loss: 1.304669, val_loss: 2.227937 ,Tr_acc: 0.958427 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 433:Tr_loss: 1.302880, val_loss: 2.230123 ,Tr_acc: 0.959176 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 434:Tr_loss: 1.301103, val_loss: 2.229147 ,Tr_acc: 0.958801 , val_acc: 0.518000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 435:Tr_loss: 1.299332, val_loss: 2.231402 ,Tr_acc: 0.959551 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 436:Tr_loss: 1.297569, val_loss: 2.230380 ,Tr_acc: 0.960300 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 437:Tr_loss: 1.295812, val_loss: 2.232683 ,Tr_acc: 0.959551 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 438:Tr_loss: 1.294065, val_loss: 2.231638 ,Tr_acc: 0.960674 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 439:Tr_loss: 1.292326, val_loss: 2.233954 ,Tr_acc: 0.960674 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 440:Tr_loss: 1.290595, val_loss: 2.232916 ,Tr_acc: 0.961798 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 441:Tr_loss: 1.288871, val_loss: 2.235250 ,Tr_acc: 0.961798 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 442:Tr_loss: 1.287154, val_loss: 2.234211 ,Tr_acc: 0.962172 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 443:Tr_loss: 1.285447, val_loss: 2.236546 ,Tr_acc: 0.961798 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 444:Tr_loss: 1.283750, val_loss: 2.235496 ,Tr_acc: 0.962547 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 445:Tr_loss: 1.282061, val_loss: 2.237953 ,Tr_acc: 0.962172 , val_acc: 0.516000 , lr: 0.050000 , Reg:0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-96cd68696d15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mcorrect_logprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mdata_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_logprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[0mreg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mW3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mall_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#max_count = 100\n",
    "#for count in range (max_count):\n",
    "    #print('search space iteration ( %d / 100)' %(count))\n",
    "#reg = 10**random.uniform(-4,0)\n",
    "#learning_rate = 10**random.uniform(-3,-6) # regularization strength\n",
    "reg = 1e-3 \n",
    "learning_rate = 5e-2\n",
    "# initialize parameters randomly\n",
    "h1 = 1000 # size of hidden layer 1\n",
    "h2 = 1000 #size of hidden layer 2 \n",
    "W = np.random.randn(D,h1)/ np.sqrt(D) #xavier\n",
    "b = np.zeros((1,h1))\n",
    "#print(b.shape)\n",
    "W2 = np.random.randn(h1,h2)/ np.sqrt(h1)\n",
    "b2 = np.zeros((1,h2))\n",
    "W3= np.random.randn(h2,K)/ np.sqrt(h2)\n",
    "b3=np.zeros((1,K))\n",
    "\n",
    "#preprocess the data\n",
    "X = training_data.astype('float64')\n",
    "y = training_labels\n",
    "\n",
    "\n",
    "X -= np.mean (X , axis = 0 )\n",
    "#X = X.astype('float')\n",
    "#print (training_data.shape)\n",
    "#X /= np.std (X , axis = 0 )\n",
    "X/= 255.0\n",
    "\n",
    "validation_d = validation_data.astype('float64')\n",
    "validation_d -= np.mean (validation_data , axis = 0 )\n",
    "#validation_data /= np.std (validation_data , axis = 0 )\n",
    "validation_d /= 255.0\n",
    "\n",
    "# some hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "all_loss=[]\n",
    "all_validation_loss = []\n",
    "prev_val_acc = 0\n",
    "for i in range(500): #100 epochs\n",
    "\n",
    "  # evaluate class scores, [N x K]\n",
    "    hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
    "    hidden_layer2= np.maximum(0, np.dot(hidden_layer, W2) + b2) # note, ReLU activation\n",
    "    scores = np.dot(hidden_layer2, W3) + b3\n",
    "    training_predicted_class = np.argmax(scores, axis=1)\n",
    "    training_acc = np.mean(training_predicted_class == y)\n",
    "\n",
    "    #evaluate validation accuracy\n",
    "\n",
    "    validation_hidden_layer = np.maximum(0, np.dot(validation_d, W) + b) # note, ReLU activation\n",
    "    validation_hidden_layer2= np.maximum(0, np.dot(validation_hidden_layer, W2) + b2) # note, ReLU activation\n",
    "    validation_scores = np.dot(validation_hidden_layer2, W3) + b3\n",
    "\n",
    "    validation_predicted_class = np.argmax(validation_scores, axis=1)\n",
    "    validation_acc = np.mean(validation_predicted_class == validation_labels)\n",
    "\n",
    "    # compute the class probabilities\n",
    "    scores -= np.max(scores) # to avoid numerical blowup\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(correct_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2) + 0.5*reg*np.sum(W3*W3) \n",
    "    loss = data_loss + reg_loss\n",
    "    all_loss.append(loss)\n",
    "\n",
    "    # compute validation scores \n",
    "    validation_scores -= np.max(validation_scores) # to avoid numerical blowup\n",
    "    exp_validation_scores = np.exp(validation_scores)\n",
    "    '''\n",
    "    try:\n",
    "        validation_probs = exp_validation_scores / np.sum(exp_validation_scores, axis=1, keepdims=True)\n",
    "    except ZeroDivisionError:\n",
    "        print('division by zero')\n",
    "    finally:\n",
    "        validation_probs = exp_validation_scores / 1e-12\n",
    "    '''\n",
    "    validation_probs = exp_validation_scores / np.sum(exp_validation_scores, axis=1, keepdims=True) # [N x K]\n",
    "    #print (validation_probs.shape)\n",
    "\n",
    "    #compute validation loss\n",
    "    validation_num_examples = validation_data.shape[0]\n",
    "    validation_correct_logprobs = -np.log(validation_probs[range(validation_num_examples),validation_labels])\n",
    "    #print (validation_correct_logprobs.shape)\n",
    "    validation_data_loss = np.sum(validation_correct_logprobs)/validation_num_examples\n",
    "    #reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2) + 0.5*reg*np.sum(W3*W3) \n",
    "    validation_loss = validation_data_loss + reg_loss\n",
    "    all_validation_loss.append(validation_loss)\n",
    "\n",
    "    #if i % 10 == 0:\n",
    "    print (\"Epoch %d:Tr_loss: %f, val_loss: %f ,Tr_acc: %f , val_acc: %f , lr: %f , Reg:%f\" % (i, loss , validation_loss , training_acc , validation_acc , learning_rate , reg ))\n",
    "\n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "    # backpropate the gradient to the parameters\n",
    "    #first backprop into parameters W3 and b3\n",
    "    dW3 = np.dot (hidden_layer2.T , dscores)\n",
    "    db3 = np.sum(dscores, axis=0, keepdims=True)\n",
    "    # next backprop into hidden layer 2\n",
    "    dhidden_2 = np.dot(dscores, W3.T)\n",
    "    # backprop the ReLU non-linearity\n",
    "    dhidden_2[hidden_layer2 <= 0] = 0\n",
    "    # next backprop into parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, dhidden_2)\n",
    "    db2 = np.sum(dhidden_2, axis=0, keepdims=True)\n",
    "    # next backprop into hidden layer\n",
    "    dhidden = np.dot(dhidden_2, W2.T)\n",
    "    # backprop the ReLU non-linearity\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    # finally into W,b\n",
    "    dW = np.dot(X.T, dhidden)\n",
    "    db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "    # add regularization gradient contribution\n",
    "    dW3 += reg * W3\n",
    "    dW2 += reg * W2\n",
    "    dW += reg * W\n",
    "    \n",
    "    #assigning the values \n",
    "    if (validation_acc > prev_val_acc):\n",
    "        prev_val_acc = validation_acc\n",
    "        Best_W = copy.deepcopy(W)\n",
    "        Best_W2 = copy.deepcopy(W2)\n",
    "        Best_W3 = copy.deepcopy(W3)\n",
    "        Best_b = copy.deepcopy(b)\n",
    "        Best_b2 = copy.deepcopy(b2)\n",
    "        Best_b3 = copy.deepcopy(b3)\n",
    "        Best_val_acc = copy.deepcopy(validation_acc)\n",
    "\n",
    "    # perform a parameter update\n",
    "    W += -learning_rate * dW\n",
    "    b += -learning_rate * db\n",
    "    W2 += -learning_rate * dW2\n",
    "    b2 += -learning_rate * db2\n",
    "    W3 += -learning_rate * dW3\n",
    "    b3 += -learning_rate * db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_loss ,label = 'training Loss')\n",
    "plt.plot (all_validation_loss , label='validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate training set accuracy\n",
    "hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
    "hidden_layer2= np.maximum(0, np.dot(hidden_layer, W2) + b2) # note, ReLU activation\n",
    "scores = np.dot(hidden_layer2, W3) + b3\n",
    "\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print ('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate validation set accuracy\n",
    "validation_data = validation_data.astype('float64')\n",
    "#validation_data -= np.mean(validation_data , axis = 0)\n",
    "#validation_data /= np.std(validation_data , axis = 0)\n",
    "#print (Best_W.shape)\n",
    "#print (Best_W2.shape)\n",
    "#print(Best_val_acc)\n",
    "hidden_layer = np.maximum(0, np.dot(validation_data, Best_W) + Best_b) # note, ReLU activation\n",
    "hidden_layer2= np.maximum(0, np.dot(hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "scores = np.dot(hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "print(Best_val_acc)\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "#print(predicted_class[:10])\n",
    "#print (validation_labels [:10])\n",
    "print ('validation_set accuracy: %.2f' % (np.mean(predicted_class == validation_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0 \n",
    "theta = 0 \n",
    "try:\n",
    "    n = r / theta\n",
    "except ZeroDivisionError:\n",
    "    print('division by zero')\n",
    "finally:\n",
    "    n = 1\n",
    "    \n",
    "print (n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing ACCR: 55.40 %\n"
     ]
    }
   ],
   "source": [
    "#running test set and calculating the ACCR\n",
    "testing_d = testing_data.astype('float64')\n",
    "testing_d -= np.mean (testing_data , axis = 0)\n",
    "testing_d /= 255.0\n",
    "testing_hidden_layer = np.maximum(0, np.dot(testing_d, Best_W) + Best_b) # note, ReLU activation\n",
    "testing_hidden_layer2= np.maximum(0, np.dot(testing_hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "testing_scores = np.dot(testing_hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "predicted_class = np.argmax(testing_scores, axis=1)\n",
    "\n",
    "print ('testing ACCR: %.2f ' % (np.mean(predicted_class == testing_labels)*100) +'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daisy accuracy: 41.00 %\n"
     ]
    }
   ],
   "source": [
    "#calculating individual classes and calculating the CCRNA\n",
    "#daisy\n",
    "testing_d = daisy.astype('float64')\n",
    "testing_d -= np.mean (testing_data , axis = 0)\n",
    "testing_d /= 255.0\n",
    "testing_hidden_layer = np.maximum(0, np.dot(testing_d, Best_W) + Best_b) # note, ReLU activation\n",
    "testing_hidden_layer2= np.maximum(0, np.dot(testing_hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "testing_scores = np.dot(testing_hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "predicted_class = np.argmax(testing_scores, axis=1)\n",
    "\n",
    "print ('Daisy accuracy: %.2f ' % (np.mean(predicted_class == daisy_lbls)*100) +'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dandelion accuracy: 70.00 %\n"
     ]
    }
   ],
   "source": [
    "#calculating individual classes and calculating the CCRNA\n",
    "#dandelion\n",
    "testing_d = dandelion.astype('float64')\n",
    "testing_d -= np.mean (testing_data , axis = 0)\n",
    "testing_d /= 255.0\n",
    "testing_hidden_layer = np.maximum(0, np.dot(testing_d, Best_W) + Best_b) # note, ReLU activation\n",
    "testing_hidden_layer2= np.maximum(0, np.dot(testing_hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "testing_scores = np.dot(testing_hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "predicted_class = np.argmax(testing_scores, axis=1)\n",
    "\n",
    "print ('dandelion accuracy: %.2f ' % (np.mean(predicted_class == dandelion_lbls)*100) +'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roses accuracy: 38.00 %\n"
     ]
    }
   ],
   "source": [
    "#calculating individual classes and calculating the CCRNA\n",
    "#roses\n",
    "testing_d = roses.astype('float64')\n",
    "testing_d -= np.mean (testing_data , axis = 0)\n",
    "testing_d /= 255.0\n",
    "testing_hidden_layer = np.maximum(0, np.dot(testing_d, Best_W) + Best_b) # note, ReLU activation\n",
    "testing_hidden_layer2= np.maximum(0, np.dot(testing_hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "testing_scores = np.dot(testing_hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "predicted_class = np.argmax(testing_scores, axis=1)\n",
    "\n",
    "print ('roses accuracy: %.2f ' % (np.mean(predicted_class == roses_lbls)*100) +'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunflowers accuracy: 71.00 %\n"
     ]
    }
   ],
   "source": [
    "#calculating individual classes and calculating the CCRNA\n",
    "#sunflowers\n",
    "testing_d = sunflowers.astype('float64')\n",
    "testing_d -= np.mean (testing_data , axis = 0)\n",
    "testing_d /= 255.0\n",
    "testing_hidden_layer = np.maximum(0, np.dot(testing_d, Best_W) + Best_b) # note, ReLU activation\n",
    "testing_hidden_layer2= np.maximum(0, np.dot(testing_hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "testing_scores = np.dot(testing_hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "predicted_class = np.argmax(testing_scores, axis=1)\n",
    "\n",
    "print ('sunflowers accuracy: %.2f ' % (np.mean(predicted_class == sunflowers_lbls)*100) +'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tulips accuracy: 57.00 %\n"
     ]
    }
   ],
   "source": [
    "#calculating individual classes and calculating the CCRNA\n",
    "#tulips\n",
    "testing_d = tulips.astype('float64')\n",
    "testing_d -= np.mean (testing_data , axis = 0)\n",
    "testing_d /= 255.0\n",
    "testing_hidden_layer = np.maximum(0, np.dot(testing_d, Best_W) + Best_b) # note, ReLU activation\n",
    "testing_hidden_layer2= np.maximum(0, np.dot(testing_hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "testing_scores = np.dot(testing_hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "predicted_class = np.argmax(testing_scores, axis=1)\n",
    "\n",
    "print ('tulips accuracy: %.2f ' % (np.mean(predicted_class == tulips_lbls)*100) +'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the class to be more dynamic in terms of (number of layers / number of nodes in each layer /Activation functions ..)\n",
    "# To Do : Add more activation functions and gradient flavors\n",
    "class fully_connected ():\n",
    "    def __init__ (self,n_layers):\n",
    "        self.n_layers= n_layers\n",
    "        self.h = []\n",
    "        self.Best_W = {} #dictionary that has all the best W\n",
    "        self.Best_b = {} # dictionary that has the best Bias \n",
    "        self.Best_val_acc = 0\n",
    "    \n",
    "    def set_layers_nodes (self,h): #h is a numpy array of length n_layer , prints an error message if not\n",
    "        if (len(h) != self.n_layers):\n",
    "            print('the length of the node number array is not compatible with the number of hidden layers')\n",
    "        else:\n",
    "            self.h = h\n",
    "    \n",
    "    def compute(self,training_data = training_data , training_labels = training_labels ,\n",
    "                validation_data = validation_data , validation_labels = validation_labels,\n",
    "                D=3072 , K=5 , lr = 1e-0 , reg = 1e-3 , epochs = 100):\n",
    "        n_layers = self.n_layers\n",
    "        reg = reg\n",
    "        learning_rate = lr\n",
    "        h = self.h\n",
    "        weights = {}\n",
    "        bias = {}\n",
    "        \n",
    "        weights[0] = np.random.randn(D,h[0])/ np.sqrt(D)\n",
    "        bias[0]= np.zeros((1,h[0]))\n",
    "        weights[n_layers]= np.random.randn(h[n_layers-1], K)/ np.sqrt(h[n_layers-1])\n",
    "        bias[n_layers]= np.zeros((1,K))\n",
    "        \n",
    "        # initialize parameters randomly\n",
    "        for i in range (1,n_layers):\n",
    "            weights[i] = np.random.randn(h[i-1],h[i])/ np.sqrt(h[i-1])\n",
    "            bias[i]= np.zeros((1,h[i]))\n",
    "        \n",
    "        #weights = weights.transpose(2,0,1)\n",
    "        #bias = bias.transpose(2,0,1)\n",
    "        #print (weights.shape)\n",
    "        #print (bias.shape)\n",
    "        '''\n",
    "        h1 = 1000 # size of hidden layer 1\n",
    "        h2 = 1000 #size of hidden layer 2 \n",
    "        W = np.random.randn(D,h1)/ np.sqrt(D) #xavier\n",
    "        b = np.zeros((1,h1))\n",
    "        W2 = np.random.randn(h1,h2)/ np.sqrt(h1)\n",
    "        b2 = np.zeros((1,h2))\n",
    "        W3= np.random.randn(h2,K)/ np.sqrt(h2)\n",
    "        b3=np.zeros((1,K))\n",
    "        '''\n",
    "        #preprocess the data\n",
    "        X = training_data.astype('float64')\n",
    "        y = training_labels\n",
    "\n",
    "\n",
    "        X -= np.mean (X , axis = 0 )\n",
    "        #X = X.astype('float')\n",
    "        #print (training_data.shape)\n",
    "        #X /= np.std (X , axis = 0 )\n",
    "        X/= 255.0\n",
    "\n",
    "        validation_d = validation_data.astype('float64')\n",
    "        validation_d -= np.mean (validation_data , axis = 0 )\n",
    "        #validation_data /= np.std (validation_data , axis = 0 )\n",
    "        validation_d /= 255.0\n",
    "\n",
    "        # some hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "        # gradient descent loop\n",
    "        num_examples = X.shape[0]\n",
    "        all_loss=[]\n",
    "        all_validation_loss = []\n",
    "        prev_val_acc = 0\n",
    "        hidden_layers= {}\n",
    "        hidden_layers[0] = np.maximum(0, np.dot(X, weights[0]) + bias[0])\n",
    "        validation_hidden_layers={}\n",
    "        for epoch in range(epochs): #10 epochs\n",
    "\n",
    "          # evaluate class scores, [N x K]\n",
    "            #hidden_layers= np.append(hidden_layers,()\n",
    "            for i in range (1 , n_layers):\n",
    "                tmp_h = np.maximum(0, np.dot(hidden_layers[i-1], weights[i]) + bias[i])\n",
    "                #print (tmp_h.shape)\n",
    "                hidden_layers[i] =  tmp_h\n",
    "            #hidden_layers = hidden_layers.transpose(2,0,1)\n",
    "            #print (hidden_layers.shape)\n",
    "                \n",
    "                                     \n",
    "            '''\n",
    "            #hidden_layer = np.maximum(0, np.dot(X, weights[]) + b) # note, ReLU activation\n",
    "            #hidden_layer2= np.maximum(0, np.dot(hidden_layer, W2) + b2) # note, ReLU activation\n",
    "            '''\n",
    "            scores = np.dot(hidden_layers[n_layers-1], weights[n_layers]) + bias[n_layers]\n",
    "            training_predicted_class = np.argmax(scores, axis=1)\n",
    "            training_acc = np.mean(training_predicted_class == y)\n",
    "            \n",
    "            validation_hidden_layers[0]= np.maximum(0, np.dot(validation_d, weights[0]) + bias[0]) # note, ReLU activation\n",
    "            #evaluate validation accuracy\n",
    "            for i in range (1 , n_layers):\n",
    "                validation_hidden_layers[i]= np.maximum(0, np.dot(validation_hidden_layers[i-1], weights[i]) + bias[i])\n",
    "            \n",
    "            #validation_hidden_layer2= np.maximum(0, np.dot(validation_hidden_layer, W2) + b2) # note, ReLU activation\n",
    "            validation_scores = np.dot(validation_hidden_layers[n_layers-1], weights[n_layers]) + bias[n_layers]\n",
    "\n",
    "            validation_predicted_class = np.argmax(validation_scores, axis=1)\n",
    "            validation_acc = np.mean(validation_predicted_class == validation_labels)\n",
    "\n",
    "            # compute the class probabilities\n",
    "            scores -= np.max(scores) # to avoid numerical blowup\n",
    "            exp_scores = np.exp(scores)\n",
    "            probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "            # compute the loss: average cross-entropy loss and regularization\n",
    "            correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "            data_loss = np.sum(correct_logprobs)/num_examples\n",
    "            reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2) + 0.5*reg*np.sum(W3*W3) \n",
    "            loss = data_loss + reg_loss\n",
    "            all_loss.append(loss)\n",
    "\n",
    "            # compute validation scores \n",
    "            validation_scores -= np.max(validation_scores) # to avoid numerical blowup\n",
    "            exp_validation_scores = np.exp(validation_scores)\n",
    "            '''\n",
    "            try:\n",
    "                validation_probs = exp_validation_scores / np.sum(exp_validation_scores, axis=1, keepdims=True)\n",
    "            except ZeroDivisionError:\n",
    "                print('division by zero')\n",
    "            finally:\n",
    "                validation_probs = exp_validation_scores / 1e-12\n",
    "            '''\n",
    "            validation_probs = exp_validation_scores / np.sum(exp_validation_scores, axis=1, keepdims=True) # [N x K]\n",
    "            #print (validation_probs.shape)\n",
    "\n",
    "            #compute validation loss\n",
    "            validation_num_examples = validation_data.shape[0]\n",
    "            validation_correct_logprobs = -np.log(validation_probs[range(validation_num_examples),validation_labels])\n",
    "            #print (validation_correct_logprobs.shape)\n",
    "            validation_data_loss = np.sum(validation_correct_logprobs)/validation_num_examples\n",
    "            #reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2) + 0.5*reg*np.sum(W3*W3) \n",
    "            validation_loss = validation_data_loss + reg_loss\n",
    "            all_validation_loss.append(validation_loss)\n",
    "\n",
    "            #if i % 10 == 0:\n",
    "            print (\"Epoch %d:Tr_loss: %f, val_loss: %f ,Tr_acc: %f , val_acc: %f , lr: %f , Reg:%f\" % (epoch, loss , validation_loss , training_acc , validation_acc , learning_rate , reg ))\n",
    "\n",
    "            # compute the gradient on scores\n",
    "            dscores = probs\n",
    "            dscores[range(num_examples),y] -= 1\n",
    "            dscores /= num_examples\n",
    "            \n",
    "            dweights = {}\n",
    "            dbias = {}\n",
    "            dhidden = {}\n",
    "            # backpropate the gradient to the parameters\n",
    "            #first backprop into parameters W3 and b3\n",
    "            dweights[n_layers] = (np.dot (hidden_layers[n_layers-1].T , dscores))\n",
    "            dbias[n_layers] = (np.sum(dscores, axis=0, keepdims=True))\n",
    "            dhidden[n_layers-1] =(np.dot(dscores , weights[n_layers].T))\n",
    "            if (hidden_layer[n_layers-1].any() <= 0):\n",
    "                dhidden[n_layers-1] = 0\n",
    "            for i in reversed(range (1,n_layers)):\n",
    "                dweights[i] =np.dot (hidden_layers[i-1].T , dhidden[i]) \n",
    "                dbias[i]=np.sum(dhidden[i], axis=0, keepdims=True)\n",
    "                dhidden[i-1] = np.dot(dhidden[i], weights[i].T)\n",
    "                if(hidden_layers[i-1].any()<=0):\n",
    "                    dhidden[i-1]=0\n",
    "            \n",
    "            dweights[0]= np.dot(X.T , dhidden[0])\n",
    "            dbias[0]=np.sum(dhidden[0] , axis = 0 , keepdims=True)\n",
    "            \n",
    "            '''\n",
    "            dW3 = np.dot (hidden_layer2.T , dscores)\n",
    "            db3 = np.sum(dscores, axis=0, keepdims=True)\n",
    "            # next backprop into hidden layer 2\n",
    "            dhidden_2 = np.dot(dscores, W3.T)\n",
    "            # backprop the ReLU non-linearity\n",
    "            dhidden_2[hidden_layer2 <= 0] = 0\n",
    "            # next backprop into parameters W2 and b2\n",
    "            dW2 = np.dot(hidden_layer.T, dhidden_2)\n",
    "            db2 = np.sum(dhidden_2, axis=0, keepdims=True)\n",
    "            # next backprop into hidden layer\n",
    "            dhidden = np.dot(dhidden_2, W2.T)\n",
    "            # backprop the ReLU non-linearity\n",
    "            dhidden[hidden_layer <= 0] = 0\n",
    "            # finally into W,b\n",
    "            dW = np.dot(X.T, dhidden)\n",
    "            db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "            '''\n",
    "\n",
    "            # add regularization gradient contribution\n",
    "            for i in range ( 0 , n_layers+1):\n",
    "            \n",
    "                dweights[i] += reg * weights[i]\n",
    "            #dW2 += reg * W2\n",
    "            #dW += reg * W\n",
    "\n",
    "            #assigning the values \n",
    "            if (validation_acc > prev_val_acc):\n",
    "                prev_val_acc = validation_acc\n",
    "                Best_W = copy.deepcopy(weights)\n",
    "                #Best_W2 = copy.deepcopy(W2)\n",
    "                #Best_W3 = copy.deepcopy(W3)\n",
    "                Best_b = copy.deepcopy(bias)\n",
    "                #Best_b2 = copy.deepcopy(b2)\n",
    "                #Best_b3 = copy.deepcopy(b3)\n",
    "                Best_val_acc = copy.deepcopy(validation_acc)\n",
    "\n",
    "            # perform a parameter update\n",
    "            for  i in range (0 , n_layers +1 ):\n",
    "                weights[i] += -learning_rate * dweights[i]\n",
    "                bias[i] += -learning_rate * dbias[i]\n",
    "            #W += -learning_rate * dW\n",
    "            #b += -learning_rate * db\n",
    "            #W2 += -learning_rate * dW2\n",
    "            #b2 += -learning_rate * db2\n",
    "            #W3 += -learning_rate * dW3\n",
    "            #b3 += -learning_rate * db3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:Tr_loss: 2.623160, val_loss: 2.611978 ,Tr_acc: 0.159176 , val_acc: 0.208000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 1:Tr_loss: 2.621443, val_loss: 2.609308 ,Tr_acc: 0.160300 , val_acc: 0.216000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 2:Tr_loss: 2.619782, val_loss: 2.606690 ,Tr_acc: 0.159925 , val_acc: 0.216000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 3:Tr_loss: 2.618176, val_loss: 2.604124 ,Tr_acc: 0.159551 , val_acc: 0.220000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 4:Tr_loss: 2.616620, val_loss: 2.601611 ,Tr_acc: 0.160674 , val_acc: 0.216000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 5:Tr_loss: 2.615114, val_loss: 2.599151 ,Tr_acc: 0.160300 , val_acc: 0.222000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 6:Tr_loss: 2.613653, val_loss: 2.596736 ,Tr_acc: 0.161423 , val_acc: 0.228000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 7:Tr_loss: 2.612237, val_loss: 2.594366 ,Tr_acc: 0.164045 , val_acc: 0.232000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 8:Tr_loss: 2.610862, val_loss: 2.592041 ,Tr_acc: 0.162921 , val_acc: 0.244000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 9:Tr_loss: 2.609527, val_loss: 2.589762 ,Tr_acc: 0.168165 , val_acc: 0.246000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 10:Tr_loss: 2.608230, val_loss: 2.587524 ,Tr_acc: 0.170787 , val_acc: 0.254000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 11:Tr_loss: 2.606969, val_loss: 2.585326 ,Tr_acc: 0.174532 , val_acc: 0.258000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 12:Tr_loss: 2.605742, val_loss: 2.583167 ,Tr_acc: 0.177903 , val_acc: 0.264000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 13:Tr_loss: 2.604548, val_loss: 2.581041 ,Tr_acc: 0.181648 , val_acc: 0.264000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 14:Tr_loss: 2.603386, val_loss: 2.578952 ,Tr_acc: 0.183521 , val_acc: 0.272000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 15:Tr_loss: 2.602253, val_loss: 2.576898 ,Tr_acc: 0.189888 , val_acc: 0.286000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 16:Tr_loss: 2.601148, val_loss: 2.574878 ,Tr_acc: 0.196629 , val_acc: 0.300000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 17:Tr_loss: 2.600070, val_loss: 2.572888 ,Tr_acc: 0.197004 , val_acc: 0.304000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 18:Tr_loss: 2.599019, val_loss: 2.570928 ,Tr_acc: 0.201498 , val_acc: 0.314000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 19:Tr_loss: 2.597992, val_loss: 2.568998 ,Tr_acc: 0.205243 , val_acc: 0.314000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 20:Tr_loss: 2.596988, val_loss: 2.567093 ,Tr_acc: 0.208240 , val_acc: 0.306000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 21:Tr_loss: 2.596007, val_loss: 2.565215 ,Tr_acc: 0.212360 , val_acc: 0.316000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 22:Tr_loss: 2.595048, val_loss: 2.563363 ,Tr_acc: 0.216479 , val_acc: 0.318000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 23:Tr_loss: 2.594110, val_loss: 2.561536 ,Tr_acc: 0.220225 , val_acc: 0.308000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 24:Tr_loss: 2.593191, val_loss: 2.559734 ,Tr_acc: 0.224719 , val_acc: 0.306000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 25:Tr_loss: 2.592291, val_loss: 2.557956 ,Tr_acc: 0.228090 , val_acc: 0.310000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 26:Tr_loss: 2.591409, val_loss: 2.556200 ,Tr_acc: 0.228090 , val_acc: 0.306000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 27:Tr_loss: 2.590545, val_loss: 2.554469 ,Tr_acc: 0.231835 , val_acc: 0.314000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 28:Tr_loss: 2.589697, val_loss: 2.552763 ,Tr_acc: 0.235206 , val_acc: 0.312000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 29:Tr_loss: 2.588865, val_loss: 2.551082 ,Tr_acc: 0.237828 , val_acc: 0.310000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 30:Tr_loss: 2.588048, val_loss: 2.549423 ,Tr_acc: 0.239326 , val_acc: 0.304000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 31:Tr_loss: 2.587246, val_loss: 2.547788 ,Tr_acc: 0.240824 , val_acc: 0.306000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 32:Tr_loss: 2.586459, val_loss: 2.546174 ,Tr_acc: 0.246816 , val_acc: 0.308000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 33:Tr_loss: 2.585684, val_loss: 2.544582 ,Tr_acc: 0.248689 , val_acc: 0.306000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 34:Tr_loss: 2.584923, val_loss: 2.543011 ,Tr_acc: 0.250562 , val_acc: 0.314000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 35:Tr_loss: 2.584174, val_loss: 2.541461 ,Tr_acc: 0.252809 , val_acc: 0.312000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 36:Tr_loss: 2.583437, val_loss: 2.539930 ,Tr_acc: 0.253933 , val_acc: 0.312000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 37:Tr_loss: 2.582711, val_loss: 2.538414 ,Tr_acc: 0.258427 , val_acc: 0.316000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 38:Tr_loss: 2.581996, val_loss: 2.536921 ,Tr_acc: 0.257303 , val_acc: 0.312000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 39:Tr_loss: 2.581292, val_loss: 2.535446 ,Tr_acc: 0.257678 , val_acc: 0.310000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 40:Tr_loss: 2.580597, val_loss: 2.533988 ,Tr_acc: 0.258427 , val_acc: 0.304000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 41:Tr_loss: 2.579913, val_loss: 2.532549 ,Tr_acc: 0.260300 , val_acc: 0.304000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 42:Tr_loss: 2.579238, val_loss: 2.531126 ,Tr_acc: 0.262172 , val_acc: 0.306000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 43:Tr_loss: 2.578571, val_loss: 2.529720 ,Tr_acc: 0.264045 , val_acc: 0.304000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 44:Tr_loss: 2.577914, val_loss: 2.528332 ,Tr_acc: 0.265543 , val_acc: 0.312000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 45:Tr_loss: 2.577264, val_loss: 2.526964 ,Tr_acc: 0.265543 , val_acc: 0.312000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 46:Tr_loss: 2.576623, val_loss: 2.525614 ,Tr_acc: 0.267041 , val_acc: 0.314000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 47:Tr_loss: 2.575990, val_loss: 2.524282 ,Tr_acc: 0.267790 , val_acc: 0.314000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 48:Tr_loss: 2.575364, val_loss: 2.522966 ,Tr_acc: 0.270412 , val_acc: 0.310000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 49:Tr_loss: 2.574745, val_loss: 2.521663 ,Tr_acc: 0.272659 , val_acc: 0.312000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 50:Tr_loss: 2.574133, val_loss: 2.520372 ,Tr_acc: 0.274532 , val_acc: 0.314000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 51:Tr_loss: 2.573528, val_loss: 2.519095 ,Tr_acc: 0.275281 , val_acc: 0.310000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 52:Tr_loss: 2.572929, val_loss: 2.517835 ,Tr_acc: 0.276030 , val_acc: 0.314000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 53:Tr_loss: 2.572336, val_loss: 2.516587 ,Tr_acc: 0.276404 , val_acc: 0.312000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 54:Tr_loss: 2.571749, val_loss: 2.515353 ,Tr_acc: 0.277528 , val_acc: 0.306000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 55:Tr_loss: 2.571168, val_loss: 2.514131 ,Tr_acc: 0.277528 , val_acc: 0.308000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 56:Tr_loss: 2.570592, val_loss: 2.512922 ,Tr_acc: 0.277528 , val_acc: 0.308000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 57:Tr_loss: 2.570022, val_loss: 2.511725 ,Tr_acc: 0.278277 , val_acc: 0.310000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 58:Tr_loss: 2.569457, val_loss: 2.510536 ,Tr_acc: 0.280150 , val_acc: 0.312000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 59:Tr_loss: 2.568896, val_loss: 2.509361 ,Tr_acc: 0.281273 , val_acc: 0.312000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 60:Tr_loss: 2.568341, val_loss: 2.508200 ,Tr_acc: 0.283521 , val_acc: 0.312000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 61:Tr_loss: 2.567790, val_loss: 2.507047 ,Tr_acc: 0.285019 , val_acc: 0.314000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 62:Tr_loss: 2.567244, val_loss: 2.505907 ,Tr_acc: 0.284270 , val_acc: 0.320000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 63:Tr_loss: 2.566702, val_loss: 2.504778 ,Tr_acc: 0.286142 , val_acc: 0.320000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 64:Tr_loss: 2.566164, val_loss: 2.503658 ,Tr_acc: 0.285393 , val_acc: 0.322000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 65:Tr_loss: 2.565631, val_loss: 2.502548 ,Tr_acc: 0.285768 , val_acc: 0.322000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 66:Tr_loss: 2.565101, val_loss: 2.501449 ,Tr_acc: 0.285393 , val_acc: 0.322000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 67:Tr_loss: 2.564575, val_loss: 2.500360 ,Tr_acc: 0.285393 , val_acc: 0.322000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 68:Tr_loss: 2.564053, val_loss: 2.499282 ,Tr_acc: 0.286142 , val_acc: 0.320000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 69:Tr_loss: 2.563534, val_loss: 2.498217 ,Tr_acc: 0.283895 , val_acc: 0.320000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 70:Tr_loss: 2.563019, val_loss: 2.497162 ,Tr_acc: 0.284644 , val_acc: 0.320000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 71:Tr_loss: 2.562507, val_loss: 2.496117 ,Tr_acc: 0.286517 , val_acc: 0.318000 , lr: 0.006000 , Reg:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72:Tr_loss: 2.561998, val_loss: 2.495080 ,Tr_acc: 0.285768 , val_acc: 0.322000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 73:Tr_loss: 2.561493, val_loss: 2.494049 ,Tr_acc: 0.287266 , val_acc: 0.322000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 74:Tr_loss: 2.560991, val_loss: 2.493030 ,Tr_acc: 0.288015 , val_acc: 0.322000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 75:Tr_loss: 2.560491, val_loss: 2.492019 ,Tr_acc: 0.288015 , val_acc: 0.322000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 76:Tr_loss: 2.559994, val_loss: 2.491019 ,Tr_acc: 0.288764 , val_acc: 0.322000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 77:Tr_loss: 2.559501, val_loss: 2.490026 ,Tr_acc: 0.289139 , val_acc: 0.324000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 78:Tr_loss: 2.559009, val_loss: 2.489041 ,Tr_acc: 0.289513 , val_acc: 0.328000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 79:Tr_loss: 2.558521, val_loss: 2.488067 ,Tr_acc: 0.289513 , val_acc: 0.328000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 80:Tr_loss: 2.558035, val_loss: 2.487101 ,Tr_acc: 0.291011 , val_acc: 0.328000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 81:Tr_loss: 2.557551, val_loss: 2.486143 ,Tr_acc: 0.291011 , val_acc: 0.328000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 82:Tr_loss: 2.557070, val_loss: 2.485194 ,Tr_acc: 0.292135 , val_acc: 0.328000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 83:Tr_loss: 2.556591, val_loss: 2.484253 ,Tr_acc: 0.291760 , val_acc: 0.328000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 84:Tr_loss: 2.556115, val_loss: 2.483321 ,Tr_acc: 0.292884 , val_acc: 0.328000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 85:Tr_loss: 2.555641, val_loss: 2.482397 ,Tr_acc: 0.294007 , val_acc: 0.328000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 86:Tr_loss: 2.555169, val_loss: 2.481481 ,Tr_acc: 0.294007 , val_acc: 0.328000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 87:Tr_loss: 2.554699, val_loss: 2.480574 ,Tr_acc: 0.295131 , val_acc: 0.330000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 88:Tr_loss: 2.554231, val_loss: 2.479674 ,Tr_acc: 0.295506 , val_acc: 0.330000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 89:Tr_loss: 2.553765, val_loss: 2.478781 ,Tr_acc: 0.296629 , val_acc: 0.332000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 90:Tr_loss: 2.553301, val_loss: 2.477896 ,Tr_acc: 0.297004 , val_acc: 0.332000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 91:Tr_loss: 2.552839, val_loss: 2.477021 ,Tr_acc: 0.298502 , val_acc: 0.332000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 92:Tr_loss: 2.552379, val_loss: 2.476154 ,Tr_acc: 0.298502 , val_acc: 0.330000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 93:Tr_loss: 2.551921, val_loss: 2.475294 ,Tr_acc: 0.300000 , val_acc: 0.330000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 94:Tr_loss: 2.551465, val_loss: 2.474442 ,Tr_acc: 0.301124 , val_acc: 0.330000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 95:Tr_loss: 2.551010, val_loss: 2.473597 ,Tr_acc: 0.301873 , val_acc: 0.334000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 96:Tr_loss: 2.550557, val_loss: 2.472760 ,Tr_acc: 0.302996 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 97:Tr_loss: 2.550105, val_loss: 2.471932 ,Tr_acc: 0.304120 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 98:Tr_loss: 2.549656, val_loss: 2.471113 ,Tr_acc: 0.305618 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 99:Tr_loss: 2.549208, val_loss: 2.470300 ,Tr_acc: 0.305993 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 100:Tr_loss: 2.548761, val_loss: 2.469493 ,Tr_acc: 0.307116 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 101:Tr_loss: 2.548316, val_loss: 2.468693 ,Tr_acc: 0.307116 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 102:Tr_loss: 2.547873, val_loss: 2.467899 ,Tr_acc: 0.307865 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 103:Tr_loss: 2.547431, val_loss: 2.467111 ,Tr_acc: 0.307865 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 104:Tr_loss: 2.546990, val_loss: 2.466329 ,Tr_acc: 0.308240 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 105:Tr_loss: 2.546551, val_loss: 2.465553 ,Tr_acc: 0.308614 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 106:Tr_loss: 2.546113, val_loss: 2.464785 ,Tr_acc: 0.308614 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 107:Tr_loss: 2.545677, val_loss: 2.464023 ,Tr_acc: 0.310487 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 108:Tr_loss: 2.545242, val_loss: 2.463267 ,Tr_acc: 0.310112 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 109:Tr_loss: 2.544808, val_loss: 2.462516 ,Tr_acc: 0.310487 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 110:Tr_loss: 2.544376, val_loss: 2.461771 ,Tr_acc: 0.311236 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 111:Tr_loss: 2.543944, val_loss: 2.461034 ,Tr_acc: 0.310861 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 112:Tr_loss: 2.543514, val_loss: 2.460305 ,Tr_acc: 0.311236 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 113:Tr_loss: 2.543086, val_loss: 2.459583 ,Tr_acc: 0.311610 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 114:Tr_loss: 2.542658, val_loss: 2.458867 ,Tr_acc: 0.311236 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 115:Tr_loss: 2.542232, val_loss: 2.458159 ,Tr_acc: 0.311985 , val_acc: 0.342000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 116:Tr_loss: 2.541806, val_loss: 2.457456 ,Tr_acc: 0.313109 , val_acc: 0.342000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 117:Tr_loss: 2.541382, val_loss: 2.456759 ,Tr_acc: 0.313483 , val_acc: 0.342000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 118:Tr_loss: 2.540959, val_loss: 2.456066 ,Tr_acc: 0.314981 , val_acc: 0.342000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 119:Tr_loss: 2.540538, val_loss: 2.455380 ,Tr_acc: 0.315356 , val_acc: 0.342000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 120:Tr_loss: 2.540117, val_loss: 2.454700 ,Tr_acc: 0.316479 , val_acc: 0.342000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 121:Tr_loss: 2.539697, val_loss: 2.454026 ,Tr_acc: 0.317228 , val_acc: 0.342000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 122:Tr_loss: 2.539279, val_loss: 2.453357 ,Tr_acc: 0.317228 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 123:Tr_loss: 2.538861, val_loss: 2.452695 ,Tr_acc: 0.317978 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 124:Tr_loss: 2.538445, val_loss: 2.452037 ,Tr_acc: 0.317978 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 125:Tr_loss: 2.538029, val_loss: 2.451384 ,Tr_acc: 0.318727 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 126:Tr_loss: 2.537615, val_loss: 2.450736 ,Tr_acc: 0.319101 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 127:Tr_loss: 2.537201, val_loss: 2.450093 ,Tr_acc: 0.319850 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 128:Tr_loss: 2.536789, val_loss: 2.449455 ,Tr_acc: 0.320225 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 129:Tr_loss: 2.536378, val_loss: 2.448824 ,Tr_acc: 0.320599 , val_acc: 0.342000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 130:Tr_loss: 2.535967, val_loss: 2.448197 ,Tr_acc: 0.320974 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 131:Tr_loss: 2.535557, val_loss: 2.447575 ,Tr_acc: 0.321723 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 132:Tr_loss: 2.535149, val_loss: 2.446960 ,Tr_acc: 0.321723 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 133:Tr_loss: 2.534741, val_loss: 2.446350 ,Tr_acc: 0.322472 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 134:Tr_loss: 2.534334, val_loss: 2.445747 ,Tr_acc: 0.323221 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 135:Tr_loss: 2.533929, val_loss: 2.445150 ,Tr_acc: 0.323596 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 136:Tr_loss: 2.533524, val_loss: 2.444559 ,Tr_acc: 0.324719 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 137:Tr_loss: 2.533120, val_loss: 2.443973 ,Tr_acc: 0.325468 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 138:Tr_loss: 2.532716, val_loss: 2.443391 ,Tr_acc: 0.325843 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 139:Tr_loss: 2.532314, val_loss: 2.442814 ,Tr_acc: 0.326217 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 140:Tr_loss: 2.531913, val_loss: 2.442243 ,Tr_acc: 0.327341 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 141:Tr_loss: 2.531512, val_loss: 2.441677 ,Tr_acc: 0.328090 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 142:Tr_loss: 2.531112, val_loss: 2.441117 ,Tr_acc: 0.327341 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143:Tr_loss: 2.530714, val_loss: 2.440563 ,Tr_acc: 0.327715 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 144:Tr_loss: 2.530315, val_loss: 2.440014 ,Tr_acc: 0.328464 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 145:Tr_loss: 2.529918, val_loss: 2.439469 ,Tr_acc: 0.329588 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 146:Tr_loss: 2.529522, val_loss: 2.438930 ,Tr_acc: 0.329213 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 147:Tr_loss: 2.529126, val_loss: 2.438396 ,Tr_acc: 0.329588 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 148:Tr_loss: 2.528731, val_loss: 2.437867 ,Tr_acc: 0.329963 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 149:Tr_loss: 2.528337, val_loss: 2.437342 ,Tr_acc: 0.329963 , val_acc: 0.334000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 150:Tr_loss: 2.527943, val_loss: 2.436824 ,Tr_acc: 0.331086 , val_acc: 0.334000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 151:Tr_loss: 2.527551, val_loss: 2.436309 ,Tr_acc: 0.331086 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 152:Tr_loss: 2.527159, val_loss: 2.435799 ,Tr_acc: 0.331835 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 153:Tr_loss: 2.526768, val_loss: 2.435294 ,Tr_acc: 0.332959 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 154:Tr_loss: 2.526378, val_loss: 2.434793 ,Tr_acc: 0.334082 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 155:Tr_loss: 2.525988, val_loss: 2.434299 ,Tr_acc: 0.335206 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 156:Tr_loss: 2.525599, val_loss: 2.433809 ,Tr_acc: 0.334831 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 157:Tr_loss: 2.525211, val_loss: 2.433324 ,Tr_acc: 0.335206 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 158:Tr_loss: 2.524824, val_loss: 2.432843 ,Tr_acc: 0.335955 , val_acc: 0.336000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 159:Tr_loss: 2.524437, val_loss: 2.432367 ,Tr_acc: 0.336330 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 160:Tr_loss: 2.524051, val_loss: 2.431895 ,Tr_acc: 0.336330 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 161:Tr_loss: 2.523666, val_loss: 2.431430 ,Tr_acc: 0.337828 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 162:Tr_loss: 2.523281, val_loss: 2.430970 ,Tr_acc: 0.338202 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 163:Tr_loss: 2.522897, val_loss: 2.430514 ,Tr_acc: 0.338202 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 164:Tr_loss: 2.522514, val_loss: 2.430063 ,Tr_acc: 0.338951 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 165:Tr_loss: 2.522132, val_loss: 2.429617 ,Tr_acc: 0.340075 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 166:Tr_loss: 2.521750, val_loss: 2.429177 ,Tr_acc: 0.339700 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 167:Tr_loss: 2.521369, val_loss: 2.428741 ,Tr_acc: 0.340449 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 168:Tr_loss: 2.520988, val_loss: 2.428310 ,Tr_acc: 0.339700 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 169:Tr_loss: 2.520608, val_loss: 2.427883 ,Tr_acc: 0.340449 , val_acc: 0.338000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 170:Tr_loss: 2.520229, val_loss: 2.427460 ,Tr_acc: 0.340824 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 171:Tr_loss: 2.519851, val_loss: 2.427042 ,Tr_acc: 0.340824 , val_acc: 0.340000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 172:Tr_loss: 2.519473, val_loss: 2.426626 ,Tr_acc: 0.341948 , val_acc: 0.342000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 173:Tr_loss: 2.519096, val_loss: 2.426213 ,Tr_acc: 0.343446 , val_acc: 0.342000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 174:Tr_loss: 2.518720, val_loss: 2.425805 ,Tr_acc: 0.343446 , val_acc: 0.346000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 175:Tr_loss: 2.518344, val_loss: 2.425400 ,Tr_acc: 0.344569 , val_acc: 0.348000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 176:Tr_loss: 2.517969, val_loss: 2.424998 ,Tr_acc: 0.344569 , val_acc: 0.352000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 177:Tr_loss: 2.517594, val_loss: 2.424601 ,Tr_acc: 0.344195 , val_acc: 0.352000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 178:Tr_loss: 2.517220, val_loss: 2.424207 ,Tr_acc: 0.344944 , val_acc: 0.352000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 179:Tr_loss: 2.516847, val_loss: 2.423818 ,Tr_acc: 0.345318 , val_acc: 0.352000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 180:Tr_loss: 2.516474, val_loss: 2.423433 ,Tr_acc: 0.345693 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 181:Tr_loss: 2.516102, val_loss: 2.423051 ,Tr_acc: 0.345693 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 182:Tr_loss: 2.515731, val_loss: 2.422672 ,Tr_acc: 0.344944 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 183:Tr_loss: 2.515360, val_loss: 2.422297 ,Tr_acc: 0.344569 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 184:Tr_loss: 2.514990, val_loss: 2.421925 ,Tr_acc: 0.344569 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 185:Tr_loss: 2.514620, val_loss: 2.421557 ,Tr_acc: 0.345318 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 186:Tr_loss: 2.514251, val_loss: 2.421194 ,Tr_acc: 0.345693 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 187:Tr_loss: 2.513882, val_loss: 2.420834 ,Tr_acc: 0.346816 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 188:Tr_loss: 2.513515, val_loss: 2.420476 ,Tr_acc: 0.346816 , val_acc: 0.352000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 189:Tr_loss: 2.513147, val_loss: 2.420123 ,Tr_acc: 0.346816 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 190:Tr_loss: 2.512781, val_loss: 2.419773 ,Tr_acc: 0.346816 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 191:Tr_loss: 2.512415, val_loss: 2.419427 ,Tr_acc: 0.347566 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 192:Tr_loss: 2.512049, val_loss: 2.419085 ,Tr_acc: 0.347940 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 193:Tr_loss: 2.511684, val_loss: 2.418746 ,Tr_acc: 0.348689 , val_acc: 0.348000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 194:Tr_loss: 2.511320, val_loss: 2.418410 ,Tr_acc: 0.349813 , val_acc: 0.348000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 195:Tr_loss: 2.510956, val_loss: 2.418077 ,Tr_acc: 0.349813 , val_acc: 0.348000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 196:Tr_loss: 2.510593, val_loss: 2.417749 ,Tr_acc: 0.349813 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 197:Tr_loss: 2.510230, val_loss: 2.417424 ,Tr_acc: 0.351311 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 198:Tr_loss: 2.509868, val_loss: 2.417103 ,Tr_acc: 0.351685 , val_acc: 0.350000 , lr: 0.006000 , Reg:0.001000\n",
      "Epoch 199:Tr_loss: 2.509507, val_loss: 2.416784 ,Tr_acc: 0.352060 , val_acc: 0.354000 , lr: 0.006000 , Reg:0.001000\n"
     ]
    }
   ],
   "source": [
    "#Testing the dynamic class\n",
    "r = fully_connected (2)\n",
    "r.set_layers_nodes ([1000,1000])\n",
    "r.compute(lr= 6e-03 , epochs = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights= np.append(weights,(np.random.randn(D,h[i])/ np.sqrt(D) ) , axis = 0)\n",
    "r = np.array ([])\n",
    "weights = np.random.randn(3072, 1000)/ np.sqrt(3072)\n",
    "weights = np.dstack ((weights , weights))\n",
    "weights = np.dstack ((weights , weights))\n",
    "print (weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
